{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "synth_grads.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2WjXQf42w2S"
      },
      "source": [
        "Various text classification experiments.\n",
        "\n",
        "\n",
        "\n",
        "Imports and set-up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwkhpQ5JoPeC"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers\n",
        "import gensim\n",
        "import re\n",
        "import copy\n",
        "import keras.backend as k\n",
        "import sys\n",
        "import time\n",
        "import datetime, os\n",
        "import keras\n",
        "\n",
        "# TODO: actually implement distribution in the training loop\n",
        "strategy = tf.distribute.get_strategy()\n",
        "\n",
        "use_mixed_precision=False\n",
        "tf.config.run_functions_eagerly(False)\n",
        "\n",
        "is_tpu=None\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  is_tpu = True\n",
        "except ValueError:\n",
        "  is_tpu = False\n",
        "\n",
        "if is_tpu:\n",
        "  print('TPU available.')\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.TPUStrategy(tpu)\n",
        "  if use_mixed_precision:\n",
        "    policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
        "    tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "else:\n",
        "  print('No TPU available.')\n",
        "  result = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode(\"utf-8\").strip()\n",
        "  if \"has failed\" in result:\n",
        "    print(\"No GPU available.\")\n",
        "  else:\n",
        "    print(result)\n",
        "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(tf.distribute.experimental.CollectiveCommunication.NCCL)\n",
        "    if use_mixed_precision:\n",
        "      policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
        "      tf.keras.mixed_precision.experimental.set_policy(policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_E1cqntSPWP"
      },
      "source": [
        "Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5gtHENioTM7"
      },
      "source": [
        "# Download the Sentiment140 dataset\n",
        "!mkdir -p data\n",
        "!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip -P data\n",
        "!unzip -n -d data data/training.1600000.processed.noemoticon.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nL4SsjjSEg5"
      },
      "source": [
        "Loading and splitting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC4vovKhjBZc"
      },
      "source": [
        "sen140 = pd.read_csv(\n",
        "    \"data/training.1600000.processed.noemoticon.csv\", encoding='latin-1',\n",
        "    names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n",
        "sen140.head()\n",
        "sen140 = sen140.sample(frac=1).reset_index(drop=True)\n",
        "sen140 = sen140[['text', 'target']]\n",
        "features, targets = sen140.iloc[:,0].values, sen140.iloc[:,1].values\n",
        "\n",
        "print(\"A random tweet\\t:\", features[0])\n",
        "\n",
        "# split between train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(features, targets, test_size=0.33)\n",
        "# Scale classes to the [0, 1] range\n",
        "y_train = y_train.astype(\"float32\")/4.0\n",
        "y_test = y_test.astype(\"float32\")/4.0\n",
        "# Shape\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqURbOUvLQiZ"
      },
      "source": [
        "Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJKyAOFJFrSH"
      },
      "source": [
        "# Standardizing and splitting the strings.\n",
        "\n",
        "def process_tweet(x):\n",
        "  x = x.strip()\n",
        "  x = x.lower()\n",
        "  x = re.sub(r\"[^a-zA-Z0-9üöäÜÖÄß\\.,!\\?\\-%\\$€\\/ ]+'\", ' ', x) # :(\n",
        "  x = re.sub('([\\.,!\\?\\-%\\$€\\/])',r' \\1 ', x)\n",
        "  x = re.sub('\\s{2,}', ' ', x)\n",
        "  x = x.split()\n",
        "  x.append(\"[&END&]\")\n",
        "  length = len(x)\n",
        "  return x\n",
        "\n",
        "tweets_train = []\n",
        "tweets_test = []\n",
        "for tweet in x_train:\n",
        "  tweets_train.append(process_tweet(tweet[0]))\n",
        "for tweet in x_test:\n",
        "  tweets_test.append(process_tweet(tweet[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK9RSe_ytJwB"
      },
      "source": [
        "# Building the initial vocab with all words from the training set\n",
        "def add_or_update_word(_vocab, word):\n",
        "  entry = None\n",
        "  if word in _vocab:\n",
        "    entry = _vocab[word]\n",
        "    entry = (entry[0], entry[1]+1)\n",
        "  else:\n",
        "    entry = (len(_vocab), 1)\n",
        "  _vocab[word] = entry\n",
        "\n",
        "vocab_pre = {}\n",
        "# \"[&END&]\" is for padding, \"[&UNK&]\" for unknown words\n",
        "add_or_update_word(vocab_pre, \"[&END&]\")\n",
        "add_or_update_word(vocab_pre, \"[&UNK&]\")\n",
        "for tweet in tweets_train:\n",
        "  for word in tweet:\n",
        "    add_or_update_word(vocab_pre, word)\n",
        "\n",
        "\n",
        "# limiting the vocabulary to only include words that appear at least 3 times\n",
        "# in the training data set. Reduces vocab size to about 1/6th.\n",
        "# This is to make it harder for the model to overfit by focusing on words that\n",
        "# may only appear in the training data, and also to generally make it learn to \n",
        "# handle unknown words (more robust)\n",
        "keys = vocab_pre.keys()\n",
        "vocab = {}\n",
        "vocab[\"[&END&]\"] = 0\n",
        "vocab[\"[&UNK&]\"] = 1\n",
        "for key in keys:\n",
        "  freq = vocab_pre[key][1]\n",
        "  index = vocab_pre[key][0]\n",
        "  if freq >= 3 and index>1:\n",
        "    vocab[key] = len(vocab)\n",
        "\n",
        "# Replace words that have been removed from the vocabulary with \"[&UNK&]\" in\n",
        "# both the training and testing data\n",
        "def filter_unknown(_in, _vocab):\n",
        "  for tweet in _in:\n",
        "    for i in range(len(tweet)):\n",
        "      if not tweet[i] in _vocab:\n",
        "        tweet[i] = \"[&UNK&]\"\n",
        "\n",
        "filter_unknown(tweets_train, vocab)\n",
        "filter_unknown(tweets_test, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2r3KZ-jrTF8"
      },
      "source": [
        "Using gensim word2vec to get a good word embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj-of7Tx3yhR"
      },
      "source": [
        "# train the embedding. TODO: Save the result for later use, this takes some time\n",
        "embedding_dims=128\n",
        "embedding = gensim.models.Word2Vec(tweets_train, size=embedding_dims, min_count=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6nG2bkL41Y8"
      },
      "source": [
        "# convert the training and test data to their tokenized form based on the\n",
        "# word indices that gensim's word2vec decided on.\n",
        "def tokenize(_in, _vocab):\n",
        "  _out = []\n",
        "  for i in range(len(_in)):\n",
        "    tweet = _in[i]\n",
        "    wordlist = []\n",
        "    for word in tweet:\n",
        "      wordlist.append(_vocab[word].index)\n",
        "    _out.append(wordlist)\n",
        "  return _out\n",
        "\n",
        "tokens_train = tokenize(tweets_train, embedding.wv.vocab)\n",
        "tokens_test = tokenize(tweets_test, embedding.wv.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5b18iZDrUFq"
      },
      "source": [
        "Creating modules and defining the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rvavG5yVu98"
      },
      "source": [
        "class SequenceCollapseAttention(tf.Module):\n",
        "  '''\n",
        "  Collapses a sequence of arbitrary length into num_out_entries entries from the\n",
        "  sequence according to dot-product attention. A variable length sequence is\n",
        "  thus collapsed to a fixed length.\n",
        "  '''\n",
        "  def __init__(self, num_out_entries, initializer=tf.keras.initializers.HeNormal, name=None):\n",
        "      super().__init__(name=name)\n",
        "      self.is_built = False\n",
        "      self.num_out_entries = num_out_entries\n",
        "      self.initializer = initializer()\n",
        "\n",
        "  def __call__(self, keys, query):\n",
        "      if not self.is_built:\n",
        "          self.weights = tf.Variable(\n",
        "              self.initializer([query.shape[-1], self.num_out_entries]), trainable=True)\n",
        "          self.biases = tf.Variable(tf.zeros([self.num_out_entries]), trainable=True)\n",
        "          self.is_built = True\n",
        "\n",
        "      scores = tf.matmul(query, self.weights) + self.biases\n",
        "      scores = tf.transpose(scores, perm=(0,2,1))\n",
        "      scores = tf.nn.softmax(scores)\n",
        "      output = tf.linalg.matmul(scores, keys)\n",
        "      return output\n",
        "\n",
        "class WordEmbedding(tf.Module):\n",
        "  '''\n",
        "  Creates a word-embedding module from a provided embedding matrix.\n",
        "  '''\n",
        "  def __init__(self, embedding_matrix, trainable=False, name=None):\n",
        "      super().__init__(name=name)\n",
        "      self.embedding = tf.Variable(embedding_matrix, trainable=trainable)\n",
        "\n",
        "  # @tf.function #(experimental_relax_shapes=True)\n",
        "  def __call__(self, x):\n",
        "      return tf.nn.embedding_lookup(self.embedding, x)\n",
        "\n",
        "testvar = None\n",
        "\n",
        "class PositionalEncoding1D(tf.Module):\n",
        "  '''\n",
        "  Positional encoding as in the Attention Is All You Need paper. I hope.\n",
        "  '''\n",
        "  def __init__(self, axis=-2, base=1000, name=None):\n",
        "      super().__init__(name=name)\n",
        "      self.axis=axis\n",
        "      self.base=base\n",
        "      self.encoding_weight=tf.Variable([2.0], trainable=True, name=\"enc_weight\")\n",
        "      testvar = self.encoding_weight\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, x):\n",
        "    sequence_length = tf.shape(x)[self.axis]\n",
        "    d = tf.shape(x)[-1]\n",
        "    T = tf.shape(x)[self.axis]\n",
        "    pos_enc = tf.range(0, d/2, delta=1, dtype=tf.float32)\n",
        "    pos_enc = (-2.0/tf.cast(d, dtype=tf.float32))*pos_enc\n",
        "    base = tf.cast(tf.fill(tf.shape(pos_enc), self.base), dtype=tf.float32)\n",
        "    pos_enc = tf.math.pow(base, pos_enc)\n",
        "    pos_enc = tf.expand_dims(pos_enc, axis=0)\n",
        "    pos_enc = tf.tile(pos_enc, [T,1])\n",
        "    t = tf.expand_dims(tf.range(1, T+1, delta=1, dtype=tf.float32), axis=-1)\n",
        "    pos_enc = tf.math.multiply(pos_enc, t)\n",
        "    pos_enc_sin = tf.expand_dims(tf.math.sin(pos_enc), axis=-1)\n",
        "    pos_enc_cos = tf.expand_dims(tf.math.cos(pos_enc), axis=-1)\n",
        "    pos_enc = tf.concat((pos_enc_sin, pos_enc_cos), axis=-1)\n",
        "    pos_enc = tf.reshape(pos_enc, [T,d])\n",
        "    return x+(pos_enc*self.encoding_weight)\n",
        "\n",
        "class MLP_Block(tf.Module):\n",
        "  '''\n",
        "  With batch normalization before the activations.\n",
        "  A regular old multilayer perceptron, hidden shapes are defined by the \"shapes\"\n",
        "  argument.\n",
        "  '''\n",
        "  def __init__(self, shapes, initializer=tf.keras.initializers.HeNormal, name=None, activation=tf.nn.swish):\n",
        "      super().__init__(name=name)\n",
        "      self.is_built = False\n",
        "      self.shapes = shapes\n",
        "      self.initializer = initializer()\n",
        "      self.weights = [None] * len(shapes)\n",
        "      self.biases = [None] * len(shapes)\n",
        "      self.batch_norms = [None] * len(shapes)\n",
        "      self.activation = activation\n",
        "\n",
        "  def _build(self, x):\n",
        "      for n in range(0, len(self.shapes)):\n",
        "          in_shape = x.shape[-1] if n == 0 else self.shapes[n - 1]\n",
        "          factor = 1 if self.activation != tf.nn.crelu or n == 0 else 2\n",
        "          self.weights[n] = tf.Variable(\n",
        "              self.initializer([in_shape * factor, self.shapes[n]]), trainable=True)\n",
        "          self.biases[n] = tf.Variable(tf.zeros([self.shapes[n]]), trainable=True)\n",
        "          self.batch_norms[n] = layers.BatchNormalization(trainable=True)\n",
        "      self.is_built = True\n",
        "\n",
        "  def __call__(self, x):\n",
        "      if not self.is_built:\n",
        "          self._build(x)\n",
        "\n",
        "      h = x\n",
        "      for n in range(len(self.shapes)):\n",
        "          h = tf.matmul(h, self.weights[n]) + self.biases[n]\n",
        "          h = self.batch_norms[n](h)\n",
        "          h = self.activation(h)\n",
        "\n",
        "      return h\n",
        "\n",
        "class SyntheticGradient(tf.Module):\n",
        "    '''\n",
        "    An implementation of synthetic gradients. When added to a model, this\n",
        "    module will intercept incoming gradients and replace them by learned,\n",
        "    synthetic ones.\n",
        "\n",
        "    Depending on the dimensionality and magnitude of incoming gradients, the \n",
        "    chosen initializer, or activations, the gradients provided by the generator \n",
        "    might be too large in the beginning and lead to NANs.\n",
        "\n",
        "    This can be mitigated by using a uniform initializer for the generator\n",
        "    (default), training the generator for a number of epochs before\n",
        "    generating the first gradient (default 16), using a bounded activation for\n",
        "    the hidden layers of the generator (default tanh), changing the learning\n",
        "    rate of the generator, but most directly and effectively by setting an\n",
        "    output_scale for the generated gradient (default 1.0).\n",
        "\n",
        "    When the model using this module does not learn, the generator might be too\n",
        "    simple, the output_scale might be too low, the learning rate of the\n",
        "    generator might be too large or too low, or there may be a bug of which i\n",
        "    am not yet aware.\n",
        "\n",
        "    The relative_generator_hidden_shapes list defines the shapes of the hidden\n",
        "    layers of the generator as a multiple of its input dimension. For an affine\n",
        "    transormation, pass an empty list.\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 initializer=tf.keras.initializers.GlorotUniform,\n",
        "                 activation=tf.nn.tanh,\n",
        "                 relative_generator_hidden_shapes=[3,],\n",
        "                 learning_rate=0.01,\n",
        "                 first_batch_epochs=16,\n",
        "                 sg_output_scale=1.0,\n",
        "                 name=None):\n",
        "      super().__init__(name=name)\n",
        "      self.is_built = False\n",
        "      self.initializer = initializer\n",
        "      self.activation = activation\n",
        "      self.relative_generator_hidden_shapes = relative_generator_hidden_shapes\n",
        "      self.first_batch_epochs = first_batch_epochs\n",
        "      self.sg_output_scale = sg_output_scale\n",
        "      self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    def _build(self, xy, dy):\n",
        "      '''\n",
        "      Builds the gradient generator on its first run, and trains on the first\n",
        "      incoming batch of gradeints for a number of epochs to avoid bad results\n",
        "      (including NANs) in the first few batches where the generator still\n",
        "      outputs bad approximations.\n",
        "      '''\n",
        "      if len(self.relative_generator_hidden_shapes)>0:\n",
        "        generator_shape = [xy.shape[-1]*mult for mult in self.relative_generator_hidden_shapes]\n",
        "        self._generator_hidden = MLP_Block(\n",
        "            generator_shape,\n",
        "            activation=self.activation,\n",
        "            initializer=self.initializer)\n",
        "      else:\n",
        "        self._generator_hidden = tf.identity\n",
        "        \n",
        "      self._generator_out = MLP_Block(\n",
        "          [dy.shape[-1]],\n",
        "          activation=tf.identity,\n",
        "          initializer=self.initializer)\n",
        "      \n",
        "      # train for a number of epochs on the first run, by default 16, to avoid\n",
        "      # bad results in the beginning of training.\n",
        "      for i in range(self.first_batch_epochs):\n",
        "        self._model_grad(xy, dy)\n",
        "      self.is_built = True\n",
        "\n",
        "    @tf.function\n",
        "    def _grad(self, x):\n",
        "      '''\n",
        "      Just an MLP, or just an affine transformation (as suggested in that google\n",
        "      paper) if the hidden shape in the constructor is set to be empty. The\n",
        "      divisions by size are an attempt to avoid NANs caused by gradients that \n",
        "      are too large. Still, especially when using no hidden layers here or when\n",
        "      the input dimension and/or magnitude is large, the output scale needs to\n",
        "      be set to some low value by trial and error until NANs no longer occur.\n",
        "      '''\n",
        "      x = tf.stop_gradient(x)\n",
        "      x = self._generator_hidden(x)\n",
        "      x = x/tf.cast(tf.shape(x)[-1], dtype=tf.float32)\n",
        "      out = self._generator_out(x)\n",
        "      out = tf.squeeze(out)\n",
        "      out = out/tf.cast(tf.shape(out)[-1], dtype=tf.float32)\n",
        "      return out*self.sg_output_scale\n",
        "\n",
        "    @tf.function\n",
        "    def _model_grad(self, x, _target):\n",
        "      '''\n",
        "      Gradient descend for the gradient generator. This is called every time a\n",
        "      gradient comes in, although in theory (especially with deeper gradient\n",
        "      generators) once the gradients are modeled sufficiently, it could be OK to\n",
        "      stop training on incoming gradients, thus fully decoupling the lower parts \n",
        "      of the network from the upper parts relative to this SG module.\n",
        "      '''\n",
        "      with tf.GradientTape() as tape:\n",
        "          _sg = self._grad(x)\n",
        "          l2_loss = _target - _sg\n",
        "          l2_loss = tf.math.reduce_sum(tf.math.square(l2_loss), axis=-1)\n",
        "          #l2_loss = tf.math.sqrt(l2_dist)\n",
        "          grads = tape.gradient(l2_loss, self.trainable_variables)\n",
        "          self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "\n",
        "    @tf.custom_gradient\n",
        "    def sg(self, x, y):\n",
        "      '''\n",
        "      In the forward pass it is essentially a no-op (identity). In the backwards\n",
        "      pass it replaces the incoming gradient by a synthetic one.\n",
        "      '''\n",
        "      x = tf.identity(x)\n",
        "      def grad(dy):\n",
        "        # concat x and the label to be inputs for the generator:\n",
        "        xy = self.concat_x_and_y(x,y)\n",
        "\n",
        "        if not self.is_built:\n",
        "            self._build(xy, dy)\n",
        "        # train the generator on the incoming gradient:\n",
        "        self._model_grad(xy, dy)\n",
        "        \n",
        "        # return the gradient. The second return value is the gradient for y,\n",
        "        # which should be zero since we only need y (labels) to generate the \n",
        "        # synthetic gradients\n",
        "        dy = self._grad(xy)\n",
        "        return dy, tf.zeros(tf.shape(y))\n",
        "      return x, grad\n",
        "\n",
        "    def __call__(self, x, y):\n",
        "      return self.sg(x, y)\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def concat_x_and_y(self, x, y):\n",
        "      '''\n",
        "      Probably an overly complex yet incomplete solution to a rather small\n",
        "      inconvenience.\n",
        "      Inconvenience: The gradient generators take the output of the last module\n",
        "      AND the target/labels of the network as inputs. But those two tensors can\n",
        "      be of different shapes. The obvious solution would be to manually reshape\n",
        "      the targets so they can be concatenated with the outputs of the past\n",
        "      state. But because i wanted this SG module to be as \"plug-and-play\" as\n",
        "      possible, i tried to attempt automatic reshaping.\n",
        "\n",
        "      Should work for 1d->1d, and 1d-sequence -> 1d, possibly 1d seq->seq,\n",
        "      unsure about the rest.\n",
        "      '''\n",
        "      # insert as many dims before the last dim of y to give it the same rank\n",
        "      # as x\n",
        "      amount = tf.math.maximum(tf.rank(x)-tf.rank(y), 0)\n",
        "      new_shape = tf.concat((tf.shape(y)[:-1],\n",
        "                             tf.tile([1], [amount]),\n",
        "                             [tf.shape(y)[-1]]), axis=-1)\n",
        "      y = tf.reshape(y, new_shape)\n",
        "\n",
        "      # tile the added dims such that x and y can be concatenated\n",
        "      # In order to tile only the added dims, i need to set the dimensions with\n",
        "      # a length of 1 (except the last) to the length of the corresponding\n",
        "      # dimensions in x, while setting the rest to 1. This is waiting to break.\n",
        "      mask = tf.cast( tf.math.less_equal(tf.shape(y),\n",
        "                                         tf.constant([1])), dtype=tf.int32)\n",
        "      #ignore the last dim\n",
        "      mask = tf.concat([mask[:-1],tf.constant([0])],axis=-1)\n",
        "\n",
        "      zeros_to_ones = tf.math.subtract(tf.ones(tf.shape(mask), dtype=tf.int32), mask)\n",
        "      # has ones where there is a one in the shape, now the 1s are set to the\n",
        "      # length in x\n",
        "      mask = tf.math.multiply(mask,tf.shape(x))\n",
        "      # add ones to all other dimensions to preserve their shape\n",
        "      mask = tf.math.add(zeros_to_ones, mask)\n",
        "      # tile\n",
        "      y = tf.tile(y, mask)\n",
        "      return tf.concat((x, y), axis=-1)\n",
        "\n",
        "class FlattenL2D(tf.Module):\n",
        "    \"Flattens the last two dimensions only\"\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        new_shape = tf.concat(\n",
        "            (tf.shape(x)[:-2], [(tf.shape(x)[-1]) * (tf.shape(x)[-2])]),\n",
        "            axis=-1)\n",
        "        return tf.reshape(x, new_shape)\n",
        "\n",
        "\n",
        "initializer=tf.keras.initializers.HeNormal\n",
        "\n",
        "\n",
        "class SentimentAnalysisWithAttention(tf.Module):\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        # Structure and the idea behind it:\n",
        "        # 1: The input sequence is embedded and gets positional encoding.\n",
        "        # 2.1: An MLP block ('query') computes scores for the following\n",
        "        #      attention layer for each entry in the sequence. Ie, it decides\n",
        "        #      which words are worth a closer look.\n",
        "        # 2.2: And attention layer selects n positionally encoded word\n",
        "        #      embeddings from the input sequence based on the learned queries.\n",
        "        # 3: The result is flattened into a tensor of known shape and a number\n",
        "        #    of dense layers compute the final classification.\n",
        "\n",
        "        self.embedding = WordEmbedding(embedding.wv.vectors)\n",
        "        self.batch_norm = layers.BatchNormalization()\n",
        "        self.pos_enc = PositionalEncoding1D()\n",
        "        self.query = MLP_Block([512, 256, 256], initializer=initializer)\n",
        "        self.attention = SequenceCollapseAttention(num_out_entries=9,\n",
        "                                                   initializer=initializer)\n",
        "        self.flatten = FlattenL2D()\n",
        "        self.dense = MLP_Block([1024, 512, 256, 128, 64], initializer=initializer)\n",
        "        self.denseout = MLP_Block([1],\n",
        "                                  initializer=initializer,\n",
        "                                  activation=tf.nn.sigmoid)\n",
        "        \n",
        "\n",
        "        # Synthetic gradient modules for the various layers.\n",
        "        self.sg_query = SyntheticGradient(first_batch_epochs=64,\n",
        "                                          sg_output_scale=0.1,\n",
        "                                          relative_generator_hidden_shapes=[9,3])\n",
        "        self.sg_attention = SyntheticGradient()\n",
        "        self.sg_dense = SyntheticGradient()\n",
        "\n",
        "    def __call__(self, x, y=tf.constant([]), training=True):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.batch_norm(x)\n",
        "        q = self.query(x)\n",
        "        # q = self.sg_query(q, y)     #SG, commented out because it's slowing things down\n",
        "        x = self.attention(x, q)\n",
        "        x = self.flatten(x)\n",
        "        x = self.sg_attention(x, y)   #SG\n",
        "        x = self.dense(x)\n",
        "        x = self.sg_dense(x, y)       #SG\n",
        "        output = self.denseout(x)\n",
        "        return output\n",
        "\n",
        "model = SentimentAnalysisWithAttention()"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhyMycaj03Tj"
      },
      "source": [
        "class BatchGenerator(keras.utils.Sequence):\n",
        "  '''\n",
        "  This is a relic from the early days of this notepad, solving a problem i \n",
        "  no longer face, and i should probably remove this.\n",
        "  Creates batches from the given data, specifically it pads the sequences\n",
        "  per batch only as much as necessary to make every sequence withing a batch be\n",
        "  of the same length.\n",
        "  '''\n",
        "  def __init__(self, inputs, labels, padding, batch_size):\n",
        "      self.batch_size = batch_size\n",
        "      self.labels = labels\n",
        "      self.inputs = inputs\n",
        "      self.padding = padding\n",
        "      #self.on_epoch_end()\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.floor(len(self.inputs) / self.batch_size))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    max_length = 0\n",
        "    start_index = index*self.batch_size\n",
        "    end_index = start_index+self.batch_size\n",
        "    for i in range(start_index, end_index):\n",
        "      l = len(self.inputs[i])\n",
        "      if l>max_length:\n",
        "        max_length = l\n",
        "    \n",
        "    out_x = np.empty([self.batch_size, max_length], dtype='int32')\n",
        "    out_y = np.empty([self.batch_size, 1], dtype='float32')\n",
        "    for i in range(self.batch_size):\n",
        "      out_y[i] = self.labels[start_index+i]\n",
        "      tweet = self.inputs[start_index+i]\n",
        "      l = len(tweet)\n",
        "      l = min(l,max_length)\n",
        "      for j in range(0, l):\n",
        "        out_x[i][j] = tweet[j]\n",
        "      for j in range(l, max_length):\n",
        "        out_x[i][j] = self.padding\n",
        "    return out_x, out_y\n",
        "\n",
        "    #def on_epoch_end(self):\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlL1dFpqrZPc"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNJne1GoxT50"
      },
      "source": [
        "@tf.function(experimental_relax_shapes=True)\n",
        "def training_step(_model, _loss, metrics, _optimizer, _x_batch, _y_batch):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = _model(_x_batch, _y_batch)\n",
        "    losses = _loss(_y_batch, predictions)\n",
        "\n",
        "    for metric in metrics:\n",
        "      metric.update_state(_y_batch, predictions)\n",
        "\n",
        "    grads = tape.gradient(losses, _model.trainable_variables)\n",
        "    _optimizer.apply_gradients(zip(grads, _model.trainable_variables))\n",
        "  \n",
        "def fit(_epochs, _model, _loss, _metrics, _optimizer, _training_generator):\n",
        "  batch_time = -1\n",
        "  for epoch in range(_epochs):\n",
        "    out_length = 0\n",
        "    start_e = time.time()\n",
        "    start_p = time.time()\n",
        "    num_batches = len(_training_generator)\n",
        "    for b in range(num_batches):\n",
        "        start_b = time.time()\n",
        "\n",
        "        x_batch, y_batch = _training_generator[b]\n",
        "        training_step(_model, _loss, _metrics, _optimizer, x_batch, y_batch)\n",
        "\n",
        "        # progress output\n",
        "        elapsed_t = time.time()-start_b\n",
        "        if batch_time != -1:\n",
        "            batch_time = 0.05*elapsed_t + 0.95*batch_time\n",
        "        else:\n",
        "            batch_time = elapsed_t\n",
        "        if int(time.time()-start_p) >= 1 or b==(num_batches-1):\n",
        "            start_p = time.time()\n",
        "            eta = int((num_batches-b)*batch_time)\n",
        "            ela = int(time.time()-start_e)\n",
        "            if out_length != 0:\n",
        "                sys.stdout.write(\"\\b\"*(out_length))\n",
        "            out_string = \"\\nEpoch %d/%d,\\tbatch %d/%d,\\telapsed: %d/%ds\" % (\n",
        "                (epoch+1), _epochs, b+1, num_batches, ela, ela+eta)\n",
        "            for metric in _metrics:\n",
        "                out_string += \"\\t %s: %f\" % (metric.name, float(metric.result()))\n",
        "            out_length = len(out_string)\n",
        "            sys.stdout.write(out_string)\n",
        "    for metric in _metrics:\n",
        "        metric.reset_states()\n",
        "    sys.stdout.write(\"\\n\")\n",
        "\n",
        "sgd=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "optimizer = sgd\n",
        "metrics = (tf.keras.metrics.BinaryCrossentropy(from_logits=True), tf.keras.metrics.BinaryAccuracy())\n",
        "\n",
        "batch_size = 512\n",
        "epochs = 4\n",
        "padding = embedding.wv.vocab[\"[&END&]\"].index\n",
        "\n",
        "training_generator = BatchGenerator(tokens_train, y_train, padding, batch_size=batch_size)\n",
        "fit(epochs, model, loss, metrics, optimizer, training_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFZA-n8Qrdo1"
      },
      "source": [
        "Testing it on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yeCFIz6FYx_"
      },
      "source": [
        "testing_generator = BatchGenerator(tokens_test, y_test, padding, batch_size=batch_size)\n",
        "\n",
        "for metric in metrics:\n",
        "    metric.reset_states()\n",
        "\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def validation_step(_model, metrics, _x_batch, _y_batch):\n",
        "  predictions = _model(_x_batch, _y_batch, training=False)\n",
        "  for metric in metrics:\n",
        "    metric.update_state(_y_batch, predictions)\n",
        "\n",
        "def validate(_model, _metrics, _testing_generator):\n",
        "  batch_time = -1\n",
        "  out_length = 0\n",
        "  start_e = time.time()\n",
        "  start_p = time.time()\n",
        "  num_batches = len(_testing_generator)\n",
        "  for b in range(num_batches):\n",
        "      start_b = time.time()\n",
        "\n",
        "      x_batch, y_batch = _testing_generator[b]\n",
        "      validation_step(_model, _metrics, x_batch, y_batch)\n",
        "\n",
        "      # progress output\n",
        "      elapsed_t = time.time()-start_b\n",
        "      if batch_time != -1:\n",
        "          batch_time = 0.05*elapsed_t + 0.95*batch_time\n",
        "      else:\n",
        "          batch_time = elapsed_t\n",
        "      if int(time.time()-start_p) >= 1 or b==(num_batches-1):\n",
        "          start_p = time.time()\n",
        "          eta = int((num_batches-b)*batch_time)\n",
        "          ela = int(time.time()-start_e)\n",
        "          if out_length != 0:\n",
        "              sys.stdout.write(\"\\b\"*(out_length+1))\n",
        "          out_string = \"Batch %d/%d,\\telapsed: %d/%ds \" % (\n",
        "              b+1, num_batches, ela, ela+eta)\n",
        "          for metric in _metrics:\n",
        "              out_string += \"\\t %s: %f\" % (metric.name, float(metric.result()))\n",
        "          out_length = len(out_string)\n",
        "          sys.stdout.write(out_string)\n",
        "  for metric in _metrics:\n",
        "      metric.reset_states()\n",
        "\n",
        "validate(model, metrics, testing_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMqtgxDNgcOK"
      },
      "source": [
        "Get some example results from the the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKR5G4_JVuaR"
      },
      "source": [
        "@tf.function(experimental_relax_shapes=True)\n",
        "def predict_step(_model, _x_batch):\n",
        "  predictions = _model(_x_batch, training=False)\n",
        "  return predictions\n",
        "\n",
        "def predict(_model, generator):\n",
        "  num_batches = len(generator)\n",
        "  out = [None]*num_batches\n",
        "  for b in range(num_batches):\n",
        "    x_batch, y_batch = generator[b]\n",
        "    out[b] = predict_step(_model, x_batch)\n",
        "  return np.concatenate(out)\n",
        "\n",
        "most_evil_tweet=None\n",
        "most_evil_evilness=1\n",
        "most_cool_tweet=None\n",
        "most_cool_coolness=1\n",
        "most_angelic_tweet=None\n",
        "most_angelic_angelicness=0\n",
        "y_pred = predict(model, testing_generator)\n",
        "for i in range(0,len(y_pred)):\n",
        "    judgement = y_pred[i]\n",
        "    polarity = abs(judgement-0.5)*2\n",
        "\n",
        "    if judgement>=most_angelic_angelicness:\n",
        "        most_angelic_angelicness = judgement\n",
        "        most_angelic_tweet = x_test[i]\n",
        "    if judgement<=most_evil_evilness:\n",
        "        most_evil_evilness = judgement\n",
        "        most_evil_tweet = x_test[i]\n",
        "    if polarity<=most_cool_coolness:\n",
        "        most_cool_coolness = polarity\n",
        "        most_cool_tweet = x_test[i]\n",
        "\n",
        "\n",
        "print(\"The evilest tweet known to humankind:\\n\\t\", most_evil_tweet)\n",
        "print(\"Evilness: \", 1.0-most_evil_evilness)\n",
        "print(\"\\n\")\n",
        "print(\"The most angelic tweet any mortal has ever laid eyes upon:\\n\\t\", most_angelic_tweet)\n",
        "print(\"Angelicness: \", most_angelic_angelicness)\n",
        "print(\"\\n\")\n",
        "print(\"And this tweet is simply too cool for you:\\n\\t\", most_cool_tweet)\n",
        "print(\"Coolness: \", 1.0-most_cool_coolness)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
