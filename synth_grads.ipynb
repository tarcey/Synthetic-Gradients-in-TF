{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "synth_grads.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2WjXQf42w2S"
      },
      "source": [
        "Various text classification experiments.\n",
        "\n",
        "\n",
        "\n",
        "Imports and set-up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwkhpQ5JoPeC",
        "outputId": "5fe08816-173b-4870-e76e-f20aa365be42"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers\n",
        "import gensim\n",
        "import re\n",
        "import copy\n",
        "import keras.backend as k\n",
        "import sys\n",
        "import time\n",
        "import datetime, os\n",
        "import keras\n",
        "\n",
        "# TODO: actually implement distribution in the training loop\n",
        "strategy = tf.distribute.get_strategy()\n",
        "\n",
        "use_mixed_precision=False\n",
        "tf.config.run_functions_eagerly(False)\n",
        "\n",
        "is_tpu=None\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  is_tpu = True\n",
        "except ValueError:\n",
        "  is_tpu = False\n",
        "\n",
        "if is_tpu:\n",
        "  print('TPU available.')\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.TPUStrategy(tpu)\n",
        "  if use_mixed_precision:\n",
        "    policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
        "    tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "else:\n",
        "  print('No TPU available.')\n",
        "  result = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode(\"utf-8\").strip()\n",
        "  if \"has failed\" in result:\n",
        "    print(\"No GPU available.\")\n",
        "  else:\n",
        "    print(result)\n",
        "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(tf.distribute.experimental.CollectiveCommunication.NCCL)\n",
        "    if use_mixed_precision:\n",
        "      policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
        "      tf.keras.mixed_precision.experimental.set_policy(policy)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No TPU available.\n",
            "GPU 0: Tesla T4 (UUID: GPU-3734368d-8319-7999-26df-be3c30889be0)\n",
            "WARNING:tensorflow:From <ipython-input-1-c53facaceff3>:45: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use distribute.MultiWorkerMirroredStrategy instead\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/device:GPU:0',)\n",
            "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0',), communication = CommunicationImplementation.NCCL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_E1cqntSPWP"
      },
      "source": [
        "Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5gtHENioTM7",
        "outputId": "fbaa353a-fb9c-4052-d5e1-f25e310bdf5f"
      },
      "source": [
        "# Download the Sentiment140 dataset\n",
        "!mkdir -p data\n",
        "!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip -P data\n",
        "!unzip -n -d data data/training.1600000.processed.noemoticon.csv.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘data/training.1600000.processed.noemoticon.csv.zip’ already there; not retrieving.\n",
            "\n",
            "Archive:  data/training.1600000.processed.noemoticon.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nL4SsjjSEg5"
      },
      "source": [
        "Loading and splitting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC4vovKhjBZc",
        "outputId": "8e606b25-054b-40a4-abda-8d03a8014ce6"
      },
      "source": [
        "sen140 = pd.read_csv(\n",
        "    \"data/training.1600000.processed.noemoticon.csv\", encoding='latin-1',\n",
        "    names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n",
        "sen140.head()\n",
        "sen140 = sen140.sample(frac=1).reset_index(drop=True)\n",
        "sen140 = sen140[['text', 'target']]\n",
        "features, targets = sen140.iloc[:,0].values, sen140.iloc[:,1].values\n",
        "\n",
        "print(\"A random tweet\\t:\", features[0])\n",
        "\n",
        "# split between train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(features, targets, test_size=0.33)\n",
        "# Scale classes to the [0, 1] range\n",
        "y_train = y_train.astype(\"float32\")/4.0\n",
        "y_test = y_test.astype(\"float32\")/4.0\n",
        "# Shape\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A random tweet\t: Good Morning. so gutting back to college tomorrow \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqURbOUvLQiZ"
      },
      "source": [
        "Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJKyAOFJFrSH"
      },
      "source": [
        "# Standardizing and splitting the strings.\n",
        "\n",
        "def process_tweet(x):\n",
        "  x = x.strip()\n",
        "  x = x.lower()\n",
        "  x = re.sub(r\"[^a-zA-Z0-9üöäÜÖÄß\\.,!\\?\\-%\\$€\\/ ]+'\", ' ', x) # :(\n",
        "  x = re.sub('([\\.,!\\?\\-%\\$€\\/])',r' \\1 ', x)\n",
        "  x = re.sub('\\s{2,}', ' ', x)\n",
        "  x = x.split()\n",
        "  x.append(\"[&END&]\")\n",
        "  length = len(x)\n",
        "  return x\n",
        "\n",
        "tweets_train = []\n",
        "tweets_test = []\n",
        "for tweet in x_train:\n",
        "  tweets_train.append(process_tweet(tweet[0]))\n",
        "for tweet in x_test:\n",
        "  tweets_test.append(process_tweet(tweet[0]))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK9RSe_ytJwB"
      },
      "source": [
        "# Building the initial vocab with all words from the training set\n",
        "def add_or_update_word(_vocab, word):\n",
        "  entry = None\n",
        "  if word in _vocab:\n",
        "    entry = _vocab[word]\n",
        "    entry = (entry[0], entry[1]+1)\n",
        "  else:\n",
        "    entry = (len(_vocab), 1)\n",
        "  _vocab[word] = entry\n",
        "\n",
        "vocab_pre = {}\n",
        "# \"[&END&]\" is for padding, \"[&UNK&]\" for unknown words\n",
        "add_or_update_word(vocab_pre, \"[&END&]\")\n",
        "add_or_update_word(vocab_pre, \"[&UNK&]\")\n",
        "for tweet in tweets_train:\n",
        "  for word in tweet:\n",
        "    add_or_update_word(vocab_pre, word)\n",
        "\n",
        "\n",
        "# limiting the vocabulary to only include words that appear at least 3 times\n",
        "# in the training data set. Reduces vocab size to about 1/6th.\n",
        "# This is to make it harder for the model to overfit by focusing on words that\n",
        "# may only appear in the training data, and also to generally make it learn to \n",
        "# handle unknown words (more robust)\n",
        "keys = vocab_pre.keys()\n",
        "vocab = {}\n",
        "vocab[\"[&END&]\"] = 0\n",
        "vocab[\"[&UNK&]\"] = 1\n",
        "for key in keys:\n",
        "  freq = vocab_pre[key][1]\n",
        "  index = vocab_pre[key][0]\n",
        "  if freq >= 3 and index>1:\n",
        "    vocab[key] = len(vocab)\n",
        "\n",
        "# Replace words that have been removed from the vocabulary with \"[&UNK&]\" in\n",
        "# both the training and testing data\n",
        "def filter_unknown(_in, _vocab):\n",
        "  for tweet in _in:\n",
        "    for i in range(len(tweet)):\n",
        "      if not tweet[i] in _vocab:\n",
        "        tweet[i] = \"[&UNK&]\"\n",
        "\n",
        "filter_unknown(tweets_train, vocab)\n",
        "filter_unknown(tweets_test, vocab)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2r3KZ-jrTF8"
      },
      "source": [
        "Using gensim word2vec to get a good word embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj-of7Tx3yhR"
      },
      "source": [
        "# train the embedding. TODO: Save the result for later use, this takes some time\n",
        "embedding_dims=128\n",
        "embedding = gensim.models.Word2Vec(tweets_train, size=embedding_dims, min_count=0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6nG2bkL41Y8"
      },
      "source": [
        "# convert the training and test data to their tokenized form based on the\n",
        "# word indices that gensim's word2vec decided on.\n",
        "def tokenize(_in, _vocab):\n",
        "  _out = []\n",
        "  for i in range(len(_in)):\n",
        "    tweet = _in[i]\n",
        "    wordlist = []\n",
        "    for word in tweet:\n",
        "      wordlist.append(_vocab[word].index)\n",
        "    _out.append(wordlist)\n",
        "  return _out\n",
        "\n",
        "tokens_train = tokenize(tweets_train, embedding.wv.vocab)\n",
        "tokens_test = tokenize(tweets_test, embedding.wv.vocab)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5b18iZDrUFq"
      },
      "source": [
        "Creating modules and defining the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rvavG5yVu98"
      },
      "source": [
        "class SequenceCollapseAttention(tf.Module):\n",
        "  '''\n",
        "  Collapses a sequence of arbitrary length into num_out_entries entries from the\n",
        "  sequence according to dot-product attention. A variable length sequence is\n",
        "  thus collapsed to a fixed length.\n",
        "  '''\n",
        "  def __init__(self, num_out_entries, initializer=tf.keras.initializers.HeNormal, name=None):\n",
        "      super().__init__(name=name)\n",
        "      self.is_built = False\n",
        "      self.num_out_entries = num_out_entries\n",
        "      self.initializer = initializer()\n",
        "\n",
        "  def __call__(self, keys, query):\n",
        "      if not self.is_built:\n",
        "          self.weights = tf.Variable(\n",
        "              self.initializer([query.shape[-1], self.num_out_entries]), trainable=True)\n",
        "          self.biases = tf.Variable(tf.zeros([self.num_out_entries]), trainable=True)\n",
        "          self.is_built = True\n",
        "\n",
        "      scores = tf.matmul(query, self.weights) + self.biases\n",
        "      scores = tf.transpose(scores, perm=(0,2,1))\n",
        "      scores = tf.nn.softmax(scores)\n",
        "      output = tf.linalg.matmul(scores, keys)\n",
        "      return output\n",
        "\n",
        "class WordEmbedding(tf.Module):\n",
        "  '''\n",
        "  Creates a word-embedding module from a provided embedding matrix.\n",
        "  '''\n",
        "  def __init__(self, embedding_matrix, trainable=False, name=None):\n",
        "      super().__init__(name=name)\n",
        "      self.embedding = tf.Variable(embedding_matrix, trainable=trainable)\n",
        "\n",
        "  # @tf.function #(experimental_relax_shapes=True)\n",
        "  def __call__(self, x):\n",
        "      return tf.nn.embedding_lookup(self.embedding, x)\n",
        "\n",
        "class PositionalEncoding1D(tf.Module):\n",
        "  '''\n",
        "  Positional encoding as in the Attention Is All You Need paper. I hope.\n",
        "  '''\n",
        "  def __init__(self, axis=-2, base=100, name=None):\n",
        "      super().__init__(name=name)\n",
        "      self.axis=axis\n",
        "      self.base=base\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, x):\n",
        "    sequence_length = tf.shape(x)[self.axis]\n",
        "    d = tf.shape(x)[-1]\n",
        "    T = tf.shape(x)[self.axis]\n",
        "    pos_enc = tf.range(0, d/2, delta=1, dtype=tf.float32)\n",
        "    pos_enc = (-2.0/tf.cast(d, dtype=tf.float32))*pos_enc\n",
        "    base = tf.cast(tf.fill(tf.shape(pos_enc), self.base), dtype=tf.float32)\n",
        "    pos_enc = tf.math.pow(base, pos_enc)\n",
        "    pos_enc = tf.expand_dims(pos_enc, axis=0)\n",
        "    pos_enc = tf.tile(pos_enc, [T,1])\n",
        "    t = tf.expand_dims(tf.range(1, T+1, delta=1, dtype=tf.float32), axis=-1)\n",
        "    pos_enc = tf.math.multiply(pos_enc, t)\n",
        "    pos_enc_sin = tf.expand_dims(tf.math.sin(pos_enc), axis=-1)\n",
        "    pos_enc_cos = tf.expand_dims(tf.math.cos(pos_enc), axis=-1)\n",
        "    pos_enc = tf.concat((pos_enc_sin, pos_enc_cos), axis=-1)\n",
        "    pos_enc = tf.reshape(pos_enc, [T,d])\n",
        "    return x+pos_enc\n",
        "\n",
        "class MLP_Block(tf.Module):\n",
        "  '''\n",
        "  With batch normalization before the activations.\n",
        "  A regular old multilayer perceptron, hidden shapes are defined by the \"shapes\"\n",
        "  argument.\n",
        "  '''\n",
        "  def __init__(self, shapes, initializer=tf.keras.initializers.HeNormal, name=None, activation=tf.nn.swish):\n",
        "      super().__init__(name=name)\n",
        "      self.is_built = False\n",
        "      self.shapes = shapes\n",
        "      self.initializer = initializer()\n",
        "      self.weights = [None] * len(shapes)\n",
        "      self.biases = [None] * len(shapes)\n",
        "      self.batch_norms = [None] * len(shapes)\n",
        "      self.activation = activation\n",
        "\n",
        "  def _build(self, x):\n",
        "      for n in range(0, len(self.shapes)):\n",
        "          in_shape = x.shape[-1] if n == 0 else self.shapes[n - 1]\n",
        "          factor = 1 if self.activation != tf.nn.crelu or n == 0 else 2\n",
        "          self.weights[n] = tf.Variable(\n",
        "              self.initializer([in_shape * factor, self.shapes[n]]), trainable=True)\n",
        "          self.biases[n] = tf.Variable(tf.zeros([self.shapes[n]]), trainable=True)\n",
        "          self.batch_norms[n] = layers.BatchNormalization(trainable=True)\n",
        "      self.is_built = True\n",
        "\n",
        "  def __call__(self, x):\n",
        "      if not self.is_built:\n",
        "          self._build(x)\n",
        "\n",
        "      h = x\n",
        "      for n in range(len(self.shapes)):\n",
        "          h = tf.matmul(h, self.weights[n]) + self.biases[n]\n",
        "          h = self.batch_norms[n](h)\n",
        "          h = self.activation(h)\n",
        "\n",
        "      return h\n",
        "\n",
        "class SyntheticGradient(tf.Module):\n",
        "    '''\n",
        "    An implementation of synthetic gradients. When added to a model, this\n",
        "    module will intercept incoming gradients and replace them by learned,\n",
        "    synthetic ones.\n",
        "\n",
        "    Depending on the dimensionality and magnitude of incoming gradients, the \n",
        "    chosen initializer, or activations, the gradients provided by the generator \n",
        "    might be too large in the beginning and lead to NANs.\n",
        "\n",
        "    This can be mitigated by using a uniform initializer for the generator\n",
        "    (default), training the generator for a number of epochs before\n",
        "    generating the first gradient (default 16), using a bounded activation for\n",
        "    the hidden layers of the generator (default tanh), changing the learning\n",
        "    rate of the generator, but most directly and effectively by setting an\n",
        "    output_scale for the generated gradient (default 1.0).\n",
        "\n",
        "    When the model using this module does not learn, the generator might be too\n",
        "    simple, the output_scale might be too low, the learning rate of the\n",
        "    generator might be too large or too low, or there may be a bug of which i\n",
        "    am not yet aware.\n",
        "\n",
        "    The relative_generator_hidden_shapes list defines the shapes of the hidden\n",
        "    layers of the generator as a multiple of its input dimension. For an affine\n",
        "    transormation, pass an empty list.\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 initializer=tf.keras.initializers.GlorotUniform,\n",
        "                 activation=tf.nn.tanh,\n",
        "                 relative_generator_hidden_shapes=[3,],\n",
        "                 learning_rate=0.01,\n",
        "                 first_batch_epochs=16,\n",
        "                 sg_output_scale=1.0,\n",
        "                 name=None):\n",
        "      super().__init__(name=name)\n",
        "      self.is_built = False\n",
        "      self.initializer = initializer\n",
        "      self.activation = activation\n",
        "      self.relative_generator_hidden_shapes = relative_generator_hidden_shapes\n",
        "      self.first_batch_epochs = first_batch_epochs\n",
        "      self.sg_output_scale = sg_output_scale\n",
        "      self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    def _build(self, xy, dy):\n",
        "      '''\n",
        "      Builds the gradient generator on its first run, and trains on the first\n",
        "      incoming batch of gradeints for a number of epochs to avoid bad results\n",
        "      (including NANs) in the first few batches where the generator still\n",
        "      outputs bad approximations.\n",
        "      '''\n",
        "      if len(self.relative_generator_hidden_shapes)>0:\n",
        "        generator_shape = [xy.shape[-1]*mult for mult in self.relative_generator_hidden_shapes]\n",
        "        self._generator_hidden = MLP_Block(\n",
        "            generator_shape,\n",
        "            activation=self.activation,\n",
        "            initializer=self.initializer)\n",
        "      else:\n",
        "        self._generator_hidden = tf.identity\n",
        "        \n",
        "      self._generator_out = MLP_Block(\n",
        "          [dy.shape[-1]],\n",
        "          activation=tf.identity,\n",
        "          initializer=self.initializer)\n",
        "      \n",
        "      # train for a number of epochs on the first run, by default 16, to avoid\n",
        "      # bad results in the beginning of training.\n",
        "      for i in range(self.first_batch_epochs):\n",
        "        self._model_grad(xy, dy)\n",
        "      self.is_built = True\n",
        "\n",
        "    @tf.function\n",
        "    def _grad(self, x):\n",
        "      '''\n",
        "      Just an MLP, or just an affine transformation (as suggested in that google\n",
        "      paper) if the hidden shape in the constructor is set to be empty. The\n",
        "      divisions by size are an attempt to avoid NANs caused by gradients that \n",
        "      are too large. Still, especially when using no hidden layers here or when\n",
        "      the input dimension and/or magnitude is large, the output scale needs to\n",
        "      be set to some low value by trial and error until NANs no longer occur.\n",
        "      '''\n",
        "      x = tf.stop_gradient(x)\n",
        "      x = self._generator_hidden(x)\n",
        "      x = x/tf.cast(tf.shape(x)[-1], dtype=tf.float32)\n",
        "      out = self._generator_out(x)\n",
        "      out = tf.squeeze(out)\n",
        "      out = out/tf.cast(tf.shape(out)[-1], dtype=tf.float32)\n",
        "      return out*self.sg_output_scale\n",
        "\n",
        "    @tf.function\n",
        "    def _model_grad(self, x, _target):\n",
        "      '''\n",
        "      Gradient descend for the gradient generator. This is called every time a\n",
        "      gradient comes in, although in theory (especially with deeper gradient\n",
        "      generators) once the gradients are modeled sufficiently, it could be OK to\n",
        "      stop training on incoming gradients, thus fully decoupling the lower parts \n",
        "      of the network from the upper parts relative to this SG module.\n",
        "      '''\n",
        "      with tf.GradientTape() as tape:\n",
        "          _sg = self._grad(x)\n",
        "          l2_loss = _target - _sg\n",
        "          l2_loss = tf.math.reduce_sum(tf.math.square(l2_loss), axis=-1)\n",
        "          #l2_loss = tf.math.sqrt(l2_dist)\n",
        "          grads = tape.gradient(l2_loss, self.trainable_variables)\n",
        "          self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "\n",
        "    @tf.custom_gradient\n",
        "    def sg(self, x, y):\n",
        "      '''\n",
        "      In the forward pass it is essentially a no-op (identity). In the backwards\n",
        "      pass it replaces the incoming gradient by a synthetic one.\n",
        "      '''\n",
        "      x = tf.identity(x)\n",
        "      def grad(dy):\n",
        "        # concat x and the label to be inputs for the generator:\n",
        "        xy = self.concat_x_and_y(x,y)\n",
        "\n",
        "        if not self.is_built:\n",
        "            self._build(xy, dy)\n",
        "        # train the generator on the incoming gradient:\n",
        "        self._model_grad(xy, dy)\n",
        "        \n",
        "        # return the gradient. The second return value is the gradient for y,\n",
        "        # which should be zero since we only need y (labels) to generate the \n",
        "        # synthetic gradients\n",
        "        dy = self._grad(xy)\n",
        "        return dy, tf.zeros(tf.shape(y))\n",
        "      return x, grad\n",
        "\n",
        "    def __call__(self, x, y):\n",
        "      return self.sg(x, y)\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def concat_x_and_y(self, x, y):\n",
        "      '''\n",
        "      Probably an overly complex yet incomplete solution to a rather small\n",
        "      inconvenience.\n",
        "      Inconvenience: The gradient generators take the output of the last module\n",
        "      AND the target/labels of the network as inputs. But those two tensors can\n",
        "      be of different shapes. The obvious solution would be to manually reshape\n",
        "      the targets so they can be concatenated with the outputs of the past\n",
        "      state. But because i wanted this SG module to be as \"plug-and-play\" as\n",
        "      possible, i tried to attempt automatic reshaping.\n",
        "\n",
        "      Should work for 1d->1d, and 1d-sequence -> 1d, possibly 1d seq->seq,\n",
        "      unsure about the rest.\n",
        "      '''\n",
        "      # insert as many dims before the last dim of y to give it the same rank\n",
        "      # as x\n",
        "      amount = tf.math.maximum(tf.rank(x)-tf.rank(y), 0)\n",
        "      new_shape = tf.concat((tf.shape(y)[:-1],\n",
        "                             tf.tile([1], [amount]),\n",
        "                             [tf.shape(y)[-1]]), axis=-1)\n",
        "      y = tf.reshape(y, new_shape)\n",
        "\n",
        "      # tile the added dims such that x and y can be concatenated\n",
        "      # In order to tile only the added dims, i need to set the dimensions with\n",
        "      # a length of 1 (except the last) to the length of the corresponding\n",
        "      # dimensions in x, while setting the rest to 1. This is waiting to break.\n",
        "      mask = tf.cast( tf.math.less_equal(tf.shape(y),\n",
        "                                         tf.constant([1])), dtype=tf.int32)\n",
        "      #ignore the last dim\n",
        "      mask = tf.concat([mask[:-1],tf.constant([0])],axis=-1)\n",
        "\n",
        "      zeros_to_ones = tf.math.subtract(tf.ones(tf.shape(mask), dtype=tf.int32), mask)\n",
        "      # has ones where there is a one in the shape, now the 1s are set to the\n",
        "      # length in x\n",
        "      mask = tf.math.multiply(mask,tf.shape(x))\n",
        "      # add ones to all other dimensions to preserve their shape\n",
        "      mask = tf.math.add(zeros_to_ones, mask)\n",
        "      # tile\n",
        "      y = tf.tile(y, mask)\n",
        "      return tf.concat((x, y), axis=-1)\n",
        "\n",
        "class FlattenL2D(tf.Module):\n",
        "    \"Flattens the last two dimensions only\"\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        new_shape = tf.concat(\n",
        "            (tf.shape(x)[:-2], [(tf.shape(x)[-1]) * (tf.shape(x)[-2])]),\n",
        "            axis=-1)\n",
        "        return tf.reshape(x, new_shape)\n",
        "\n",
        "\n",
        "initializer=tf.keras.initializers.HeNormal\n",
        "\n",
        "\n",
        "class SentimentAnalysisWithAttention(tf.Module):\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        # Structure and the idea behind it:\n",
        "        # 1: The input sequence is embedded and gets positional encoding.\n",
        "        # 2.1: An MLP block ('query') computes scores for the following\n",
        "        #      attention layer for each entry in the sequence. Ie, it decides\n",
        "        #      which words are worth a closer look.\n",
        "        # 2.2: And attention layer selects n positionally encoded word\n",
        "        #      embeddings from the input sequence based on the learned queries.\n",
        "        # 3: The result is flattened into a tensor of known shape and a number\n",
        "        #    of dense layers compute the final classification.\n",
        "\n",
        "        self.embedding = WordEmbedding(embedding.wv.vectors)\n",
        "        self.batch_norm = layers.BatchNormalization()\n",
        "        self.pos_enc = PositionalEncoding1D()\n",
        "        self.query = MLP_Block([256, 128], initializer=initializer)\n",
        "        self.attention = SequenceCollapseAttention(num_out_entries=8,\n",
        "                                                   initializer=initializer)\n",
        "        self.flatten = FlattenL2D()\n",
        "        self.dense = MLP_Block([512, 256, 128, 64], initializer=initializer)\n",
        "        self.denseout = MLP_Block([1],\n",
        "                                  initializer=initializer,\n",
        "                                  activation=tf.nn.sigmoid)\n",
        "        \n",
        "\n",
        "        # Synthetic gradient modules for the various layers.\n",
        "        self.sg_query = SyntheticGradient(first_batch_epochs=64,\n",
        "                                          sg_output_scale=0.1,\n",
        "                                          relative_generator_hidden_shapes=[9,3])\n",
        "        self.sg_attention = SyntheticGradient()\n",
        "        self.sg_dense = SyntheticGradient()\n",
        "\n",
        "    def __call__(self, x, y=tf.constant([]), training=True):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.batch_norm(x)\n",
        "        q = self.query(x)\n",
        "        # q = self.sg_query(q, y)     #SG, commented out because it's slowing things down\n",
        "        x = self.attention(x, q)\n",
        "        x = self.flatten(x)\n",
        "        x = self.sg_attention(x, y)   #SG\n",
        "        x = self.dense(x)\n",
        "        x = self.sg_dense(x, y)       #SG\n",
        "        output = self.denseout(x)\n",
        "        return output\n",
        "\n",
        "model = SentimentAnalysisWithAttention()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhyMycaj03Tj"
      },
      "source": [
        "class BatchGenerator(keras.utils.Sequence):\n",
        "  '''\n",
        "  This is a relic from the early days of this notepad, solving a problem i \n",
        "  no longer face, and i should probably remove this.\n",
        "  Creates batches from the given data, specifically it pads the sequences\n",
        "  per batch only as much as necessary to make every sequence withing a batch be\n",
        "  of the same length.\n",
        "  '''\n",
        "  def __init__(self, inputs, labels, padding, batch_size):\n",
        "      self.batch_size = batch_size\n",
        "      self.labels = labels\n",
        "      self.inputs = inputs\n",
        "      self.padding = padding\n",
        "      #self.on_epoch_end()\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.floor(len(self.inputs) / self.batch_size))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    max_length = 0\n",
        "    start_index = index*self.batch_size\n",
        "    end_index = start_index+self.batch_size\n",
        "    for i in range(start_index, end_index):\n",
        "      l = len(self.inputs[i])\n",
        "      if l>max_length:\n",
        "        max_length = l\n",
        "    \n",
        "    out_x = np.empty([self.batch_size, max_length], dtype='int32')\n",
        "    out_y = np.empty([self.batch_size, 1], dtype='float32')\n",
        "    for i in range(self.batch_size):\n",
        "      out_y[i] = self.labels[start_index+i]\n",
        "      tweet = self.inputs[start_index+i]\n",
        "      l = len(tweet)\n",
        "      l = min(l,max_length)\n",
        "      for j in range(0, l):\n",
        "        out_x[i][j] = tweet[j]\n",
        "      for j in range(l, max_length):\n",
        "        out_x[i][j] = self.padding\n",
        "    return out_x, out_y\n",
        "\n",
        "    #def on_epoch_end(self):\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlL1dFpqrZPc"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNJne1GoxT50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a451709-9647-4c60-a41d-efda6fc677ff"
      },
      "source": [
        "@tf.function(experimental_relax_shapes=True)\n",
        "def training_step(_model, _loss, metrics, _optimizer, _x_batch, _y_batch):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = _model(_x_batch, _y_batch)\n",
        "    losses = _loss(_y_batch, predictions)\n",
        "\n",
        "    for metric in metrics:\n",
        "      metric.update_state(_y_batch, predictions)\n",
        "\n",
        "    grads = tape.gradient(losses, _model.trainable_variables)\n",
        "    _optimizer.apply_gradients(zip(grads, _model.trainable_variables))\n",
        "  \n",
        "def fit(_epochs, _model, _loss, _metrics, _optimizer, _training_generator):\n",
        "  batch_time = -1\n",
        "  for epoch in range(_epochs):\n",
        "    out_length = 0\n",
        "    start_e = time.time()\n",
        "    start_p = time.time()\n",
        "    num_batches = len(_training_generator)\n",
        "    for b in range(num_batches):\n",
        "        start_b = time.time()\n",
        "\n",
        "        x_batch, y_batch = _training_generator[b]\n",
        "        training_step(_model, _loss, _metrics, _optimizer, x_batch, y_batch)\n",
        "\n",
        "        # progress output\n",
        "        elapsed_t = time.time()-start_b\n",
        "        if batch_time != -1:\n",
        "            batch_time = 0.05*elapsed_t + 0.95*batch_time\n",
        "        else:\n",
        "            batch_time = elapsed_t\n",
        "        if int(time.time()-start_p) >= 1 or b==(num_batches-1):\n",
        "            start_p = time.time()\n",
        "            eta = int((num_batches-b)*batch_time)\n",
        "            ela = int(time.time()-start_e)\n",
        "            if out_length != 0:\n",
        "                sys.stdout.write(\"\\b\"*(out_length))\n",
        "            out_string = \"\\nEpoch %d/%d,\\tbatch %d/%d,\\telapsed: %d/%ds \" % (\n",
        "                (epoch+1), _epochs, b+1, num_batches, ela, ela+eta)\n",
        "            for metric in _metrics:\n",
        "                out_string += \"\\t %s: %f\" % (metric.name, float(metric.result()))\n",
        "            out_length = len(out_string)\n",
        "            sys.stdout.write(out_string)\n",
        "    for metric in _metrics:\n",
        "        metric.reset_states()\n",
        "    sys.stdout.write(\"\\n\")\n",
        "\n",
        "sgd=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "optimizer = sgd\n",
        "metrics = (tf.keras.metrics.BinaryCrossentropy(from_logits=True), tf.keras.metrics.BinaryAccuracy())\n",
        "\n",
        "batch_size = 512\n",
        "epochs = 4\n",
        "padding = embedding.wv.vocab[\"[&END&]\"].index\n",
        "\n",
        "training_generator = BatchGenerator(tokens_train, y_train, padding, batch_size=batch_size)\n",
        "fit(epochs, model, loss, metrics, optimizer, training_generator)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'batch_normalization/gamma:0', 'batch_normalization/beta:0', 'Variable:0', 'Variable:0', 'batch_normalization_1/gamma:0', 'batch_normalization_1/beta:0', 'Variable:0', 'Variable:0', 'batch_normalization/gamma:0', 'batch_normalization/beta:0', 'Variable:0', 'Variable:0', 'batch_normalization_1/gamma:0', 'batch_normalization_1/beta:0'] when minimizing the loss.\n",
            "\n",
            "Epoch 1/4,\tbatch 1/2093,\telapsed: 3/8284s \t binary_crossentropy: 0.746263\t binary_accuracy: 0.531250WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'batch_normalization/gamma:0', 'batch_normalization/beta:0', 'Variable:0', 'Variable:0', 'batch_normalization_1/gamma:0', 'batch_normalization_1/beta:0', 'Variable:0', 'Variable:0', 'batch_normalization/gamma:0', 'batch_normalization/beta:0', 'Variable:0', 'Variable:0', 'batch_normalization_1/gamma:0', 'batch_normalization_1/beta:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'batch_normalization/gamma:0', 'batch_normalization/beta:0', 'Variable:0', 'Variable:0', 'batch_normalization_1/gamma:0', 'batch_normalization_1/beta:0', 'Variable:0', 'Variable:0', 'batch_normalization/gamma:0', 'batch_normalization/beta:0', 'Variable:0', 'Variable:0', 'batch_normalization_1/gamma:0', 'batch_normalization_1/beta:0'] when minimizing the loss.\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 4/2093,\telapsed: 5/7286s \t binary_crossentropy: 0.750028\t binary_accuracy: 0.512207\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 51/2093,\telapsed: 6/682s \t binary_crossentropy: 0.702559\t binary_accuracy: 0.502413\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 99/2093,\telapsed: 7/101s \t binary_crossentropy: 0.694660\t binary_accuracy: 0.499980\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 148/2093,\telapsed: 8/51s \t binary_crossentropy: 0.684925\t binary_accuracy: 0.530485\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 198/2093,\telapsed: 9/47s \t binary_crossentropy: 0.674385\t binary_accuracy: 0.566663\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 246/2093,\telapsed: 10/48s \t binary_crossentropy: 0.667555\t binary_accuracy: 0.590288\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 293/2093,\telapsed: 11/49s \t binary_crossentropy: 0.662410\t binary_accuracy: 0.607835\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 343/2093,\telapsed: 12/47s \t binary_crossentropy: 0.657611\t binary_accuracy: 0.622540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 392/2093,\telapsed: 13/47s \t binary_crossentropy: 0.654051\t binary_accuracy: 0.633724\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 442/2093,\telapsed: 14/47s \t binary_crossentropy: 0.650843\t binary_accuracy: 0.643051\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 491/2093,\telapsed: 15/47s \t binary_crossentropy: 0.648664\t binary_accuracy: 0.650279\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 538/2093,\telapsed: 16/49s \t binary_crossentropy: 0.646472\t binary_accuracy: 0.656301\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 588/2093,\telapsed: 18/48s \t binary_crossentropy: 0.644499\t binary_accuracy: 0.661860\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 637/2093,\telapsed: 19/48s \t binary_crossentropy: 0.642731\t binary_accuracy: 0.667098\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 686/2093,\telapsed: 20/48s \t binary_crossentropy: 0.641174\t binary_accuracy: 0.671286\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 735/2093,\telapsed: 21/48s \t binary_crossentropy: 0.639866\t binary_accuracy: 0.675125\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 785/2093,\telapsed: 22/48s \t binary_crossentropy: 0.638631\t binary_accuracy: 0.678545\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 834/2093,\telapsed: 23/49s \t binary_crossentropy: 0.637593\t binary_accuracy: 0.681542\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 883/2093,\telapsed: 24/49s \t binary_crossentropy: 0.636671\t binary_accuracy: 0.684324\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 932/2093,\telapsed: 25/49s \t binary_crossentropy: 0.635845\t binary_accuracy: 0.686725\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 979/2093,\telapsed: 26/49s \t binary_crossentropy: 0.634971\t binary_accuracy: 0.689200\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1026/2093,\telapsed: 27/50s \t binary_crossentropy: 0.634238\t binary_accuracy: 0.691353\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1074/2093,\telapsed: 28/49s \t binary_crossentropy: 0.633533\t binary_accuracy: 0.693381\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1122/2093,\telapsed: 29/49s \t binary_crossentropy: 0.633053\t binary_accuracy: 0.694940\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1171/2093,\telapsed: 30/48s \t binary_crossentropy: 0.632422\t binary_accuracy: 0.696612\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1215/2093,\telapsed: 31/51s \t binary_crossentropy: 0.631907\t binary_accuracy: 0.698118\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1261/2093,\telapsed: 32/50s \t binary_crossentropy: 0.631408\t binary_accuracy: 0.699247\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1310/2093,\telapsed: 33/49s \t binary_crossentropy: 0.630935\t binary_accuracy: 0.700889\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1358/2093,\telapsed: 34/49s \t binary_crossentropy: 0.630444\t binary_accuracy: 0.702345\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1406/2093,\telapsed: 35/49s \t binary_crossentropy: 0.629945\t binary_accuracy: 0.703907\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1454/2093,\telapsed: 36/49s \t binary_crossentropy: 0.629521\t binary_accuracy: 0.705273\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1500/2093,\telapsed: 37/49s \t binary_crossentropy: 0.629068\t binary_accuracy: 0.706525\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1544/2093,\telapsed: 38/50s \t binary_crossentropy: 0.628789\t binary_accuracy: 0.707375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1591/2093,\telapsed: 39/50s \t binary_crossentropy: 0.628400\t binary_accuracy: 0.708501\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1639/2093,\telapsed: 40/49s \t binary_crossentropy: 0.627997\t binary_accuracy: 0.709640\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1687/2093,\telapsed: 41/49s \t binary_crossentropy: 0.627594\t binary_accuracy: 0.710723\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1734/2093,\telapsed: 42/49s \t binary_crossentropy: 0.627270\t binary_accuracy: 0.711536\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1781/2093,\telapsed: 43/49s \t binary_crossentropy: 0.626970\t binary_accuracy: 0.712380\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1828/2093,\telapsed: 44/49s \t binary_crossentropy: 0.626655\t binary_accuracy: 0.713204\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1876/2093,\telapsed: 45/49s \t binary_crossentropy: 0.626324\t binary_accuracy: 0.714091\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1924/2093,\telapsed: 46/49s \t binary_crossentropy: 0.626043\t binary_accuracy: 0.714869\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 1971/2093,\telapsed: 47/49s \t binary_crossentropy: 0.625789\t binary_accuracy: 0.715564\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 2019/2093,\telapsed: 48/49s \t binary_crossentropy: 0.625497\t binary_accuracy: 0.716275\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 2065/2093,\telapsed: 49/49s \t binary_crossentropy: 0.625208\t binary_accuracy: 0.717041\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 1/4,\tbatch 2093/2093,\telapsed: 49/49s \t binary_crossentropy: 0.625051\t binary_accuracy: 0.717526\n",
            "\n",
            "Epoch 2/4,\tbatch 48/2093,\telapsed: 1/44s \t binary_crossentropy: 0.611531\t binary_accuracy: 0.753947\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 95/2093,\telapsed: 2/45s \t binary_crossentropy: 0.611138\t binary_accuracy: 0.753454\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 143/2093,\telapsed: 3/44s \t binary_crossentropy: 0.611969\t binary_accuracy: 0.751175\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 192/2093,\telapsed: 4/43s \t binary_crossentropy: 0.611209\t binary_accuracy: 0.752909\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 239/2093,\telapsed: 5/45s \t binary_crossentropy: 0.611435\t binary_accuracy: 0.753228\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 285/2093,\telapsed: 6/45s \t binary_crossentropy: 0.611806\t binary_accuracy: 0.753008\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 333/2093,\telapsed: 7/44s \t binary_crossentropy: 0.611690\t binary_accuracy: 0.753044\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 381/2093,\telapsed: 8/44s \t binary_crossentropy: 0.612001\t binary_accuracy: 0.752409\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 431/2093,\telapsed: 9/42s \t binary_crossentropy: 0.611870\t binary_accuracy: 0.752846\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 480/2093,\telapsed: 10/43s \t binary_crossentropy: 0.611820\t binary_accuracy: 0.753597\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 528/2093,\telapsed: 11/43s \t binary_crossentropy: 0.611707\t binary_accuracy: 0.753610\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 575/2093,\telapsed: 12/44s \t binary_crossentropy: 0.611327\t binary_accuracy: 0.754046\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 624/2093,\telapsed: 13/43s \t binary_crossentropy: 0.611188\t binary_accuracy: 0.754366\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 673/2093,\telapsed: 14/43s \t binary_crossentropy: 0.610956\t binary_accuracy: 0.754530\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 721/2093,\telapsed: 15/43s \t binary_crossentropy: 0.610879\t binary_accuracy: 0.754768\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 771/2093,\telapsed: 16/42s \t binary_crossentropy: 0.610754\t binary_accuracy: 0.754917\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 821/2093,\telapsed: 17/42s \t binary_crossentropy: 0.610670\t binary_accuracy: 0.755131\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 869/2093,\telapsed: 18/43s \t binary_crossentropy: 0.610508\t binary_accuracy: 0.755342\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 918/2093,\telapsed: 19/43s \t binary_crossentropy: 0.610440\t binary_accuracy: 0.755708\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 966/2093,\telapsed: 20/43s \t binary_crossentropy: 0.610171\t binary_accuracy: 0.756126\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1013/2093,\telapsed: 21/44s \t binary_crossentropy: 0.610085\t binary_accuracy: 0.756276\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1061/2093,\telapsed: 22/43s \t binary_crossentropy: 0.609922\t binary_accuracy: 0.756620\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1110/2093,\telapsed: 23/43s \t binary_crossentropy: 0.609872\t binary_accuracy: 0.756688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1159/2093,\telapsed: 24/42s \t binary_crossentropy: 0.609762\t binary_accuracy: 0.756992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1205/2093,\telapsed: 25/45s \t binary_crossentropy: 0.609527\t binary_accuracy: 0.757558\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1251/2093,\telapsed: 26/44s \t binary_crossentropy: 0.609421\t binary_accuracy: 0.757688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1300/2093,\telapsed: 27/43s \t binary_crossentropy: 0.609316\t binary_accuracy: 0.758160\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1349/2093,\telapsed: 28/43s \t binary_crossentropy: 0.609130\t binary_accuracy: 0.758713\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1397/2093,\telapsed: 29/43s \t binary_crossentropy: 0.608986\t binary_accuracy: 0.759131\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1445/2093,\telapsed: 30/44s \t binary_crossentropy: 0.608787\t binary_accuracy: 0.759686\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1493/2093,\telapsed: 31/43s \t binary_crossentropy: 0.608554\t binary_accuracy: 0.760250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1539/2093,\telapsed: 32/43s \t binary_crossentropy: 0.608444\t binary_accuracy: 0.760471\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1587/2093,\telapsed: 33/43s \t binary_crossentropy: 0.608314\t binary_accuracy: 0.760753\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1635/2093,\telapsed: 34/43s \t binary_crossentropy: 0.608103\t binary_accuracy: 0.761261\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1684/2093,\telapsed: 35/43s \t binary_crossentropy: 0.607919\t binary_accuracy: 0.761781\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1732/2093,\telapsed: 36/43s \t binary_crossentropy: 0.607785\t binary_accuracy: 0.762018\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1780/2093,\telapsed: 37/43s \t binary_crossentropy: 0.607642\t binary_accuracy: 0.762366\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1828/2093,\telapsed: 38/43s \t binary_crossentropy: 0.607502\t binary_accuracy: 0.762602\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1877/2093,\telapsed: 39/43s \t binary_crossentropy: 0.607351\t binary_accuracy: 0.762957\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1926/2093,\telapsed: 40/43s \t binary_crossentropy: 0.607219\t binary_accuracy: 0.763330\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 1974/2093,\telapsed: 41/43s \t binary_crossentropy: 0.607116\t binary_accuracy: 0.763572\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 2022/2093,\telapsed: 42/43s \t binary_crossentropy: 0.606945\t binary_accuracy: 0.763924\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 2069/2093,\telapsed: 43/43s \t binary_crossentropy: 0.606828\t binary_accuracy: 0.764215\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/4,\tbatch 2093/2093,\telapsed: 43/43s \t binary_crossentropy: 0.606776\t binary_accuracy: 0.764331\n",
            "\n",
            "Epoch 3/4,\tbatch 49/2093,\telapsed: 1/43s \t binary_crossentropy: 0.600463\t binary_accuracy: 0.779098\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 97/2093,\telapsed: 2/44s \t binary_crossentropy: 0.600174\t binary_accuracy: 0.777988\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 145/2093,\telapsed: 3/43s \t binary_crossentropy: 0.600315\t binary_accuracy: 0.777721\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 194/2093,\telapsed: 4/42s \t binary_crossentropy: 0.599770\t binary_accuracy: 0.779025\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 242/2093,\telapsed: 5/43s \t binary_crossentropy: 0.600066\t binary_accuracy: 0.779773\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 289/2093,\telapsed: 6/44s \t binary_crossentropy: 0.600388\t binary_accuracy: 0.779648\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 338/2093,\telapsed: 7/43s \t binary_crossentropy: 0.600303\t binary_accuracy: 0.779545\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 386/2093,\telapsed: 8/43s \t binary_crossentropy: 0.600510\t binary_accuracy: 0.779256\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 436/2093,\telapsed: 9/42s \t binary_crossentropy: 0.600374\t binary_accuracy: 0.779274\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 485/2093,\telapsed: 10/43s \t binary_crossentropy: 0.600632\t binary_accuracy: 0.779555\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 532/2093,\telapsed: 11/45s \t binary_crossentropy: 0.600468\t binary_accuracy: 0.779631\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 581/2093,\telapsed: 12/43s \t binary_crossentropy: 0.600228\t binary_accuracy: 0.779872\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 630/2093,\telapsed: 13/43s \t binary_crossentropy: 0.600041\t binary_accuracy: 0.780221\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 679/2093,\telapsed: 14/42s \t binary_crossentropy: 0.599914\t binary_accuracy: 0.780203\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 728/2093,\telapsed: 15/43s \t binary_crossentropy: 0.599721\t binary_accuracy: 0.780483\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 778/2093,\telapsed: 16/42s \t binary_crossentropy: 0.599648\t binary_accuracy: 0.780540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 828/2093,\telapsed: 17/43s \t binary_crossentropy: 0.599644\t binary_accuracy: 0.780667\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 876/2093,\telapsed: 18/43s \t binary_crossentropy: 0.599640\t binary_accuracy: 0.780666\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 925/2093,\telapsed: 19/43s \t binary_crossentropy: 0.599664\t binary_accuracy: 0.780652\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 973/2093,\telapsed: 20/43s \t binary_crossentropy: 0.599506\t binary_accuracy: 0.781031\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1020/2093,\telapsed: 21/44s \t binary_crossentropy: 0.599531\t binary_accuracy: 0.780854\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1068/2093,\telapsed: 22/43s \t binary_crossentropy: 0.599510\t binary_accuracy: 0.780928\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1117/2093,\telapsed: 23/43s \t binary_crossentropy: 0.599611\t binary_accuracy: 0.780753\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1167/2093,\telapsed: 24/43s \t binary_crossentropy: 0.599590\t binary_accuracy: 0.780758\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1212/2093,\telapsed: 25/45s \t binary_crossentropy: 0.599545\t binary_accuracy: 0.780839\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1258/2093,\telapsed: 26/44s \t binary_crossentropy: 0.599520\t binary_accuracy: 0.780831\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1307/2093,\telapsed: 27/43s \t binary_crossentropy: 0.599545\t binary_accuracy: 0.780972\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1355/2093,\telapsed: 28/43s \t binary_crossentropy: 0.599450\t binary_accuracy: 0.781247\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1403/2093,\telapsed: 29/43s \t binary_crossentropy: 0.599452\t binary_accuracy: 0.781441\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1451/2093,\telapsed: 30/43s \t binary_crossentropy: 0.599378\t binary_accuracy: 0.781759\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1498/2093,\telapsed: 31/43s \t binary_crossentropy: 0.599294\t binary_accuracy: 0.781911\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1543/2093,\telapsed: 32/44s \t binary_crossentropy: 0.599291\t binary_accuracy: 0.781939\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1590/2093,\telapsed: 33/44s \t binary_crossentropy: 0.599278\t binary_accuracy: 0.781949\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1638/2093,\telapsed: 34/43s \t binary_crossentropy: 0.599204\t binary_accuracy: 0.782203\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1687/2093,\telapsed: 35/43s \t binary_crossentropy: 0.599193\t binary_accuracy: 0.782265\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1735/2093,\telapsed: 36/43s \t binary_crossentropy: 0.599150\t binary_accuracy: 0.782281\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1782/2093,\telapsed: 37/43s \t binary_crossentropy: 0.599132\t binary_accuracy: 0.782346\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1830/2093,\telapsed: 38/43s \t binary_crossentropy: 0.599125\t binary_accuracy: 0.782312\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1878/2093,\telapsed: 39/43s \t binary_crossentropy: 0.599078\t binary_accuracy: 0.782390\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1927/2093,\telapsed: 40/43s \t binary_crossentropy: 0.599054\t binary_accuracy: 0.782523\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 1975/2093,\telapsed: 41/43s \t binary_crossentropy: 0.599036\t binary_accuracy: 0.782576\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 2022/2093,\telapsed: 42/43s \t binary_crossentropy: 0.598976\t binary_accuracy: 0.782666\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 2069/2093,\telapsed: 43/43s \t binary_crossentropy: 0.598955\t binary_accuracy: 0.782732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/4,\tbatch 2093/2093,\telapsed: 43/43s \t binary_crossentropy: 0.598947\t binary_accuracy: 0.782782\n",
            "\n",
            "Epoch 4/4,\tbatch 49/2093,\telapsed: 1/43s \t binary_crossentropy: 0.596561\t binary_accuracy: 0.787548\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 96/2093,\telapsed: 2/44s \t binary_crossentropy: 0.596401\t binary_accuracy: 0.787577\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 144/2093,\telapsed: 3/43s \t binary_crossentropy: 0.596732\t binary_accuracy: 0.787218\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 193/2093,\telapsed: 4/43s \t binary_crossentropy: 0.596464\t binary_accuracy: 0.787565\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 241/2093,\telapsed: 5/44s \t binary_crossentropy: 0.596724\t binary_accuracy: 0.788212\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 287/2093,\telapsed: 6/45s \t binary_crossentropy: 0.597141\t binary_accuracy: 0.787579\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 335/2093,\telapsed: 7/44s \t binary_crossentropy: 0.596973\t binary_accuracy: 0.787296\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 383/2093,\telapsed: 8/44s \t binary_crossentropy: 0.597267\t binary_accuracy: 0.786559\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 432/2093,\telapsed: 9/42s \t binary_crossentropy: 0.597183\t binary_accuracy: 0.786553\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 481/2093,\telapsed: 10/42s \t binary_crossentropy: 0.597396\t binary_accuracy: 0.786837\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 529/2093,\telapsed: 11/43s \t binary_crossentropy: 0.597267\t binary_accuracy: 0.786814\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 577/2093,\telapsed: 12/43s \t binary_crossentropy: 0.596952\t binary_accuracy: 0.787221\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 626/2093,\telapsed: 13/44s \t binary_crossentropy: 0.596859\t binary_accuracy: 0.787618\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 675/2093,\telapsed: 14/43s \t binary_crossentropy: 0.596660\t binary_accuracy: 0.787807\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 724/2093,\telapsed: 15/43s \t binary_crossentropy: 0.596522\t binary_accuracy: 0.788140\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 773/2093,\telapsed: 16/43s \t binary_crossentropy: 0.596460\t binary_accuracy: 0.788140\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 823/2093,\telapsed: 17/43s \t binary_crossentropy: 0.596517\t binary_accuracy: 0.788049\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 872/2093,\telapsed: 18/43s \t binary_crossentropy: 0.596566\t binary_accuracy: 0.787954\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 921/2093,\telapsed: 19/42s \t binary_crossentropy: 0.596621\t binary_accuracy: 0.787945\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 969/2093,\telapsed: 20/43s \t binary_crossentropy: 0.596463\t binary_accuracy: 0.788168\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1016/2093,\telapsed: 21/44s \t binary_crossentropy: 0.596528\t binary_accuracy: 0.788011\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1064/2093,\telapsed: 22/43s \t binary_crossentropy: 0.596489\t binary_accuracy: 0.788047\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1113/2093,\telapsed: 23/43s \t binary_crossentropy: 0.596605\t binary_accuracy: 0.787760\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1162/2093,\telapsed: 24/43s \t binary_crossentropy: 0.596639\t binary_accuracy: 0.787751\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1207/2093,\telapsed: 25/45s \t binary_crossentropy: 0.596586\t binary_accuracy: 0.787896\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1253/2093,\telapsed: 26/44s \t binary_crossentropy: 0.596567\t binary_accuracy: 0.787917\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1302/2093,\telapsed: 27/43s \t binary_crossentropy: 0.596617\t binary_accuracy: 0.788005\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1350/2093,\telapsed: 28/44s \t binary_crossentropy: 0.596582\t binary_accuracy: 0.788194\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1398/2093,\telapsed: 29/43s \t binary_crossentropy: 0.596580\t binary_accuracy: 0.788296\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1446/2093,\telapsed: 30/44s \t binary_crossentropy: 0.596539\t binary_accuracy: 0.788468\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1493/2093,\telapsed: 31/43s \t binary_crossentropy: 0.596490\t binary_accuracy: 0.788667\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1539/2093,\telapsed: 32/43s \t binary_crossentropy: 0.596487\t binary_accuracy: 0.788705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1586/2093,\telapsed: 33/43s \t binary_crossentropy: 0.596523\t binary_accuracy: 0.788604\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1634/2093,\telapsed: 34/43s \t binary_crossentropy: 0.596476\t binary_accuracy: 0.788734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1683/2093,\telapsed: 35/43s \t binary_crossentropy: 0.596449\t binary_accuracy: 0.788793\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1731/2093,\telapsed: 36/43s \t binary_crossentropy: 0.596438\t binary_accuracy: 0.788793\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1779/2093,\telapsed: 37/43s \t binary_crossentropy: 0.596450\t binary_accuracy: 0.788812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1827/2093,\telapsed: 38/43s \t binary_crossentropy: 0.596445\t binary_accuracy: 0.788726\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1876/2093,\telapsed: 39/43s \t binary_crossentropy: 0.596438\t binary_accuracy: 0.788756\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1925/2093,\telapsed: 40/43s \t binary_crossentropy: 0.596455\t binary_accuracy: 0.788770\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 1973/2093,\telapsed: 41/43s \t binary_crossentropy: 0.596445\t binary_accuracy: 0.788782\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 2021/2093,\telapsed: 42/43s \t binary_crossentropy: 0.596418\t binary_accuracy: 0.788805\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 2069/2093,\telapsed: 43/43s \t binary_crossentropy: 0.596414\t binary_accuracy: 0.788829\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/4,\tbatch 2093/2093,\telapsed: 43/43s \t binary_crossentropy: 0.596411\t binary_accuracy: 0.788861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFZA-n8Qrdo1"
      },
      "source": [
        "Testing it on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yeCFIz6FYx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b6dec19-3ef0-4851-9b3b-da796ce7f077"
      },
      "source": [
        "testing_generator = BatchGenerator(tokens_test, y_test, padding, batch_size=batch_size)\n",
        "\n",
        "for metric in metrics:\n",
        "    metric.reset_states()\n",
        "\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def validation_step(_model, metrics, _x_batch, _y_batch):\n",
        "  predictions = _model(_x_batch, _y_batch, training=False)\n",
        "  for metric in metrics:\n",
        "    metric.update_state(_y_batch, predictions)\n",
        "\n",
        "def validate(_model, _metrics, _testing_generator):\n",
        "  batch_time = -1\n",
        "  out_length = 0\n",
        "  start_e = time.time()\n",
        "  start_p = time.time()\n",
        "  num_batches = len(_testing_generator)\n",
        "  for b in range(num_batches):\n",
        "      start_b = time.time()\n",
        "\n",
        "      x_batch, y_batch = _testing_generator[b]\n",
        "      validation_step(_model, _metrics, x_batch, y_batch)\n",
        "\n",
        "      # progress output\n",
        "      elapsed_t = time.time()-start_b\n",
        "      if batch_time != -1:\n",
        "          batch_time = 0.05*elapsed_t + 0.95*batch_time\n",
        "      else:\n",
        "          batch_time = elapsed_t\n",
        "      if int(time.time()-start_p) >= 1 or b==(num_batches-1):\n",
        "          start_p = time.time()\n",
        "          eta = int((num_batches-b)*batch_time)\n",
        "          ela = int(time.time()-start_e)\n",
        "          if out_length != 0:\n",
        "              sys.stdout.write(\"\\b\"*(out_length+1))\n",
        "          out_string = \"Batch %d/%d,\\telapsed: %d/%ds \" % (\n",
        "              b+1, num_batches, ela, ela+eta)\n",
        "          for metric in _metrics:\n",
        "              out_string += \"\\t %s: %f\" % (metric.name, float(metric.result()))\n",
        "          out_length = len(out_string)\n",
        "          sys.stdout.write(out_string)\n",
        "  for metric in _metrics:\n",
        "      metric.reset_states()\n",
        "\n",
        "validate(model, metrics, testing_generator)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch 23/1031,\telapsed: 1/115s \t binary_crossentropy: 0.597208\t binary_accuracy: 0.792120\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bBatch 117/1031,\telapsed: 2/12s \t binary_crossentropy: 0.596090\t binary_accuracy: 0.788962\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bBatch 213/1031,\telapsed: 3/11s \t binary_crossentropy: 0.596657\t binary_accuracy: 0.788036\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bBatch 310/1031,\telapsed: 4/11s \t binary_crossentropy: 0.596614\t binary_accuracy: 0.789088\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bBatch 404/1031,\telapsed: 5/11s \t binary_crossentropy: 0.597216\t binary_accuracy: 0.788714\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bBatch 502/1031,\telapsed: 6/11s \t binary_crossentropy: 0.596570\t binary_accuracy: 0.789183\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bBatch 598/1031,\telapsed: 7/11s \t binary_crossentropy: 0.596279\t binary_accuracy: 0.789579\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bBatch 695/1031,\telapsed: 8/11s \t binary_crossentropy: 0.596094\t binary_accuracy: 0.789689\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bBatch 790/1031,\telapsed: 9/11s \t binary_crossentropy: 0.596146\t binary_accuracy: 0.789540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bBatch 888/1031,\telapsed: 10/11s \t binary_crossentropy: 0.596089\t binary_accuracy: 0.789694\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bBatch 983/1031,\telapsed: 11/11s \t binary_crossentropy: 0.596042\t binary_accuracy: 0.789553\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bBatch 1031/1031,\telapsed: 11/11s \t binary_crossentropy: 0.595908\t binary_accuracy: 0.789623"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMqtgxDNgcOK"
      },
      "source": [
        "Get some example results from the the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKR5G4_JVuaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4f6f232-c4bc-4355-cd03-4ad4d4267cb1"
      },
      "source": [
        "@tf.function(experimental_relax_shapes=True)\n",
        "def predict_step(_model, _x_batch):\n",
        "  predictions = _model(_x_batch, training=False)\n",
        "  return predictions\n",
        "\n",
        "def predict(_model, generator):\n",
        "  num_batches = len(generator)\n",
        "  out = [None]*num_batches\n",
        "  for b in range(num_batches):\n",
        "    x_batch, y_batch = generator[b]\n",
        "    out[b] = predict_step(_model, x_batch)\n",
        "  return np.concatenate(out)\n",
        "\n",
        "most_evil_tweet=None\n",
        "most_evil_evilness=1\n",
        "most_cool_tweet=None\n",
        "most_cool_coolness=1\n",
        "most_angelic_tweet=None\n",
        "most_angelic_angelicness=0\n",
        "y_pred = predict(model, testing_generator)\n",
        "for i in range(0,len(y_pred)):\n",
        "    judgement = y_pred[i]\n",
        "    polarity = abs(judgement-0.5)*2\n",
        "\n",
        "    if judgement>=most_angelic_angelicness:\n",
        "        most_angelic_angelicness = judgement\n",
        "        most_angelic_tweet = x_test[i]\n",
        "    if judgement<=most_evil_evilness:\n",
        "        most_evil_evilness = judgement\n",
        "        most_evil_tweet = x_test[i]\n",
        "    if polarity<=most_cool_coolness:\n",
        "        most_cool_coolness = polarity\n",
        "        most_cool_tweet = x_test[i]\n",
        "\n",
        "\n",
        "print(\"The evilest tweet known to humankind:\\n\\t\", most_evil_tweet)\n",
        "print(\"Evilness: \", 1.0-most_evil_evilness)\n",
        "print(\"\\n\")\n",
        "print(\"The most angelic tweet any mortal has ever laid eyes upon:\\n\\t\", most_angelic_tweet)\n",
        "print(\"Angelicness: \", most_angelic_angelicness)\n",
        "print(\"\\n\")\n",
        "print(\"And this tweet is simply too cool for you:\\n\\t\", most_cool_tweet)\n",
        "print(\"Coolness: \", 1.0-most_cool_coolness)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The evilest tweet known to humankind:\n",
            "\t ['SAD to miss Ink n Iron    I miss LA so bad !!']\n",
            "Evilness:  [1.]\n",
            "\n",
            "\n",
            "The most angelic tweet any mortal has ever laid eyes upon:\n",
            "\t ['@featureBlend Thanks  Have a great weekend bro!']\n",
            "Angelicness:  [1.]\n",
            "\n",
            "\n",
            "And this tweet is simply too cool for you:\n",
            "\t [\"@maltesk As national representative for the Netherlands I'm quite ashamed  \"]\n",
            "Coolness:  [0.9999981]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}