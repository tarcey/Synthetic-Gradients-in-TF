{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "synth_grads.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2WjXQf42w2S"
      },
      "source": [
        "Text classification with attention and synthetic gradients.\n",
        "\n",
        "\n",
        "\n",
        "Imports and set-up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwkhpQ5JoPeC"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gensim\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "\n",
        "# TODO: actually implement distribution in the training loop\n",
        "strategy = tf.distribute.get_strategy()\n",
        "\n",
        "use_mixed_precision = False\n",
        "tf.config.run_functions_eagerly(False)\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "is_tpu = None\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    is_tpu = True\n",
        "except ValueError:\n",
        "    is_tpu = False\n",
        "\n",
        "if is_tpu:\n",
        "    print('TPU available.')\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    if use_mixed_precision:\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
        "        tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "else:\n",
        "    print('No TPU available.')\n",
        "    result = subprocess.run(\n",
        "        ['nvidia-smi', '-L'],\n",
        "        stdout=subprocess.PIPE).stdout.decode(\"utf-8\").strip()\n",
        "    if \"has failed\" in result:\n",
        "        print(\"No GPU available.\")\n",
        "    else:\n",
        "        print(result)\n",
        "        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
        "            tf.distribute.experimental.CollectiveCommunication.NCCL)\n",
        "        if use_mixed_precision:\n",
        "            policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
        "            tf.keras.mixed_precision.experimental.set_policy(policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_E1cqntSPWP"
      },
      "source": [
        "Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5gtHENioTM7"
      },
      "source": [
        "# Download the Sentiment140 dataset\n",
        "!mkdir -p data\n",
        "!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip -P data\n",
        "!unzip -n -d data data/training.1600000.processed.noemoticon.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nL4SsjjSEg5"
      },
      "source": [
        "Loading and splitting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC4vovKhjBZc"
      },
      "source": [
        "sen140 = pd.read_csv(\n",
        "    \"data/training.1600000.processed.noemoticon.csv\", encoding='latin-1',\n",
        "    names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n",
        "sen140.head()\n",
        "sen140 = sen140.sample(frac=1).reset_index(drop=True)\n",
        "sen140 = sen140[['text', 'target']]\n",
        "features, targets = sen140.iloc[:, 0].values, sen140.iloc[:, 1].values\n",
        "\n",
        "print(\"A random tweet\\t:\", features[0])\n",
        "\n",
        "# split between train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(features,\n",
        "                                                    targets,\n",
        "                                                    test_size=0.33)\n",
        "y_train = y_train.astype(\"float32\") / 4.0\n",
        "y_test = y_test.astype(\"float32\") / 4.0\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqURbOUvLQiZ"
      },
      "source": [
        "Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJKyAOFJFrSH"
      },
      "source": [
        "def process_tweet(x):\n",
        "    x = x.strip()\n",
        "    x = x.lower()\n",
        "    x = re.sub(r\"[^a-zA-Z0-9üöäÜÖÄß\\.,!\\?\\-%\\$€\\/ ]+'\", ' ', x)\n",
        "    x = re.sub('([\\.,!\\?\\-%\\$€\\/])', r' \\1 ', x)\n",
        "    x = re.sub('\\s{2,}', ' ', x)\n",
        "    x = x.split()\n",
        "    x.append(\"[&END&]\")\n",
        "    length = len(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "tweets_train = []\n",
        "tweets_test = []\n",
        "for tweet in x_train:\n",
        "    tweets_train.append(process_tweet(tweet[0]))\n",
        "for tweet in x_test:\n",
        "    tweets_test.append(process_tweet(tweet[0]))\n",
        "\n",
        "\n",
        "# Building the initial vocab with all words from the training set\n",
        "def add_or_update_word(_vocab, word):\n",
        "    entry = None\n",
        "    if word in _vocab:\n",
        "        entry = _vocab[word]\n",
        "        entry = (entry[0], entry[1] + 1)\n",
        "    else:\n",
        "        entry = (len(_vocab), 1)\n",
        "    _vocab[word] = entry\n",
        "\n",
        "\n",
        "vocab_pre = {}\n",
        "# \"[&END&]\" is for padding, \"[&UNK&]\" for unknown words\n",
        "add_or_update_word(vocab_pre, \"[&END&]\")\n",
        "add_or_update_word(vocab_pre, \"[&UNK&]\")\n",
        "for tweet in tweets_train:\n",
        "    for word in tweet:\n",
        "        add_or_update_word(vocab_pre, word)\n",
        "\n",
        "# limiting the vocabulary to only include words that appear at least 3 times\n",
        "# in the training data set. Reduces vocab size to about 1/6th.\n",
        "# This is to make it harder for the model to overfit by focusing on words that\n",
        "# may only appear in the training data, and also to generally make it learn to\n",
        "# handle unknown words (more robust)\n",
        "keys = vocab_pre.keys()\n",
        "vocab = {}\n",
        "vocab[\"[&END&]\"] = 0\n",
        "vocab[\"[&UNK&]\"] = 1\n",
        "for key in keys:\n",
        "    freq = vocab_pre[key][1]\n",
        "    index = vocab_pre[key][0]\n",
        "    if freq >= 3 and index > 1:\n",
        "        vocab[key] = len(vocab)\n",
        "\n",
        "\n",
        "# Replace words that have been removed from the vocabulary with \"[&UNK&]\" in\n",
        "# both the training and testing data\n",
        "def filter_unknown(_in, _vocab):\n",
        "    for tweet in _in:\n",
        "        for i in range(len(tweet)):\n",
        "            if not tweet[i] in _vocab:\n",
        "                tweet[i] = \"[&UNK&]\"\n",
        "\n",
        "\n",
        "filter_unknown(tweets_train, vocab)\n",
        "filter_unknown(tweets_test, vocab)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2r3KZ-jrTF8"
      },
      "source": [
        "Using gensim word2vec to get a good word embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj-of7Tx3yhR"
      },
      "source": [
        "# train the embedding\n",
        "embedding_dims = 128\n",
        "embedding = gensim.models.Word2Vec(tweets_train,\n",
        "                                   size=embedding_dims, min_count=0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6nG2bkL41Y8"
      },
      "source": [
        "def tokenize(_in, _vocab):\n",
        "    _out = []\n",
        "    for i in range(len(_in)):\n",
        "        tweet = _in[i]\n",
        "        wordlist = []\n",
        "        for word in tweet:\n",
        "            wordlist.append(_vocab[word].index)\n",
        "        _out.append(wordlist)\n",
        "    return _out\n",
        "\n",
        "\n",
        "tokens_train = tokenize(tweets_train, embedding.wv.vocab)\n",
        "tokens_test = tokenize(tweets_test, embedding.wv.vocab)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5b18iZDrUFq"
      },
      "source": [
        "Creating modules and defining the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rvavG5yVu98"
      },
      "source": [
        "class SequenceCollapseAttention(tf.Module):\n",
        "    '''\n",
        "    Collapses a sequence of arbitrary length into num_out_entries entries from \n",
        "    the sequence according to dot-product attention. So, a variable length \n",
        "    sequence is reduced to a sequence of a fixed, known length.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_out_entries,\n",
        "                 initializer=tf.keras.initializers.HeNormal,\n",
        "                 name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.is_built = False\n",
        "        self.num_out_entries = num_out_entries\n",
        "        self.initializer = initializer()\n",
        "\n",
        "    def __call__(self, keys, query):\n",
        "        if not self.is_built:\n",
        "            self.weights = tf.Variable(\n",
        "                self.initializer([query.shape[-1], self.num_out_entries]),\n",
        "                trainable=True)\n",
        "            self.biases = tf.Variable(tf.zeros([self.num_out_entries]),\n",
        "                                      trainable=True)\n",
        "            self.is_built = True\n",
        "\n",
        "        scores = tf.linalg.matmul(query, self.weights) + self.biases\n",
        "        scores = tf.transpose(scores, perm=(0, 2, 1))\n",
        "        scores = tf.nn.softmax(scores)\n",
        "        output = tf.linalg.matmul(scores, keys)\n",
        "        return output\n",
        "\n",
        "\n",
        "class WordEmbedding(tf.Module):\n",
        "    '''\n",
        "    Creates a word-embedding module from a provided embedding matrix.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, embedding_matrix, trainable=False, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.embedding = tf.Variable(embedding_matrix, trainable=trainable)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return tf.nn.embedding_lookup(self.embedding, x)\n",
        "\n",
        "\n",
        "testvar = None\n",
        "\n",
        "\n",
        "class PositionalEncoding1D(tf.Module):\n",
        "    '''\n",
        "    Positional encoding as in the Attention Is All You Need paper. I hope.\n",
        "\n",
        "    For experimentation, the weight by which the positional information is mixed\n",
        "    into the input vectors is learned.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, axis=-2, base=1000, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.axis = axis\n",
        "        self.base = base\n",
        "        self.encoding_weight = tf.Variable([2.0], trainable=True)\n",
        "        testvar = self.encoding_weight\n",
        "\n",
        "    def __call__(self, x):\n",
        "        sequence_length = tf.shape(x)[self.axis]\n",
        "        d = tf.shape(x)[-1]\n",
        "        T = tf.shape(x)[self.axis]\n",
        "        pos_enc = tf.range(0, d / 2, delta=1, dtype=tf.float32)\n",
        "        pos_enc = (-2.0 / tf.cast(d, dtype=tf.float32)) * pos_enc\n",
        "        base = tf.cast(tf.fill(tf.shape(pos_enc), self.base), dtype=tf.float32)\n",
        "        pos_enc = tf.math.pow(base, pos_enc)\n",
        "        pos_enc = tf.expand_dims(pos_enc, axis=0)\n",
        "        pos_enc = tf.tile(pos_enc, [T, 1])\n",
        "        t = tf.expand_dims(tf.range(1, T+1, delta=1, dtype=tf.float32), axis=-1)\n",
        "        pos_enc = tf.math.multiply(pos_enc, t)\n",
        "        pos_enc_sin = tf.expand_dims(tf.math.sin(pos_enc), axis=-1)\n",
        "        pos_enc_cos = tf.expand_dims(tf.math.cos(pos_enc), axis=-1)\n",
        "        pos_enc = tf.concat((pos_enc_sin, pos_enc_cos), axis=-1)\n",
        "        pos_enc = tf.reshape(pos_enc, [T, d])\n",
        "        return x + (pos_enc * self.encoding_weight)\n",
        "\n",
        "\n",
        "class MLP_Block(tf.Module):\n",
        "    '''\n",
        "    With batch normalization before the activations.\n",
        "    A regular old multilayer perceptron, hidden shapes are defined by the\n",
        "    \"shapes\" argument.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 shapes,\n",
        "                 initializer=tf.keras.initializers.HeNormal,\n",
        "                 name=None,\n",
        "                 activation=tf.nn.swish,\n",
        "                 trainable_batch_norms=False):\n",
        "        super().__init__(name=name)\n",
        "        self.is_built = False\n",
        "        self.shapes = shapes\n",
        "        self.initializer = initializer()\n",
        "        self.weights = [None] * len(shapes)\n",
        "        self.biases = [None] * len(shapes)\n",
        "        self.bnorms = [None] * len(shapes)\n",
        "        self.activation = activation\n",
        "        self.trainable_batch_norms = trainable_batch_norms\n",
        "\n",
        "    def _build(self, x):\n",
        "        for n in range(0, len(self.shapes)):\n",
        "            in_shape = x.shape[-1] if n == 0 else self.shapes[n - 1]\n",
        "            factor = 1 if self.activation != tf.nn.crelu or n == 0 else 2\n",
        "            self.weights[n] = tf.Variable(\n",
        "                self.initializer([in_shape * factor, self.shapes[n]]),\n",
        "                trainable=True)\n",
        "            self.biases[n] = tf.Variable(tf.zeros([self.shapes[n]]),\n",
        "                                         trainable=True)\n",
        "            self.bnorms[n] = tf.keras.layers.BatchNormalization(\n",
        "                trainable=self.trainable_batch_norms)\n",
        "        self.is_built = True\n",
        "\n",
        "    def __call__(self, x, training=False):\n",
        "        if not self.is_built:\n",
        "            self._build(x)\n",
        "\n",
        "        h = x\n",
        "        for n in range(len(self.shapes)):\n",
        "            h = tf.linalg.matmul(h, self.weights[n]) + self.biases[n]\n",
        "            h = self.bnorms[n](h, training=training)\n",
        "            h = self.activation(h)\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "class SyntheticGradient(tf.Module):\n",
        "    '''\n",
        "    An implementation of synthetic gradients. When added to a model, this\n",
        "    module will intercept incoming gradients and replace them by learned,\n",
        "    synthetic ones.\n",
        "\n",
        "    If you encounter NANs, try setting the sg_output_scale parameter to a lower\n",
        "    value, or increase the number of initial_epochs or epochs.\n",
        "\n",
        "    When the model using this module does not learn, the generator might be too\n",
        "    simple, the sg_output_scale might be too low, the learning rate of the\n",
        "    generator might be too large or too low, or the number of epochs might be\n",
        "    too large or too low.\n",
        "\n",
        "    If the number of initial epochs is too large, the generator can get stuck\n",
        "    in a local minimum and fail to learn.\n",
        "\n",
        "    The relative_generator_hidden_shapes list defines the shapes of the hidden\n",
        "    layers of the generator as a multiple of its input dimension. For an affine\n",
        "    transormation, pass an empty list.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 initializer=tf.keras.initializers.GlorotUniform,\n",
        "                 activation=tf.nn.tanh,\n",
        "                 relative_generator_hidden_shapes=[6, ],\n",
        "                 learning_rate=0.01,\n",
        "                 epochs=1,\n",
        "                 initial_epochs=16,\n",
        "                 sg_output_scale=1,\n",
        "                 name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.is_built = False\n",
        "        self.initializer = initializer\n",
        "        self.activation = activation\n",
        "        self.relative_generator_hidden_shapes = relative_generator_hidden_shapes\n",
        "        self.initial_epochs = initial_epochs\n",
        "        self.epochs = epochs\n",
        "        self.sg_output_scale = sg_output_scale\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    def build(self, xy, dy):\n",
        "        '''\n",
        "        Builds the gradient generator on its first run, and trains on the first\n",
        "        incoming batch of gradients for a number of epochs to avoid bad results\n",
        "        (including NANs) in the first few batches where the generator still\n",
        "        outputs bad approximations. To further reduce NANs due to bad gradients,\n",
        "        a fixed scaler for the outputs of the generator is computed based on the\n",
        "        first batch.\n",
        "        '''\n",
        "        if self.is_built:\n",
        "            return\n",
        "\n",
        "        if len(self.relative_generator_hidden_shapes) > 0:\n",
        "            generator_shape = [\n",
        "                               xy.shape[-1] * mult\n",
        "                               for mult in\n",
        "                               self.relative_generator_hidden_shapes]\n",
        "            self.generator_hidden = MLP_Block(\n",
        "                generator_shape,\n",
        "                activation=self.activation,\n",
        "                initializer=self.initializer,\n",
        "                trainable_batch_norms=False)\n",
        "        else:\n",
        "            self.generator_hidden = tf.identity\n",
        "\n",
        "        self.generator_out = MLP_Block(\n",
        "            [dy.shape[-1]],\n",
        "            activation=tf.identity,\n",
        "            initializer=self.initializer,\n",
        "            trainable_batch_norms=False)\n",
        "\n",
        "        # calculate a static scaler for the generated gradients to avoid\n",
        "        # overflows due to too large gradients\n",
        "        self.generator_out_scale = 1.0\n",
        "        x = self.generate_gradient(xy) / self.sg_output_scale\n",
        "        mag_y = tf.math.sqrt(tf.math.reduce_sum(tf.math.square(dy), axis=-1))\n",
        "        mag_x = tf.math.sqrt(tf.math.reduce_sum(tf.math.square(x), axis=-1))\n",
        "        mag_scale = tf.math.reduce_mean(mag_y / mag_x,\n",
        "                                        axis=tf.range(0, tf.rank(dy) - 1))\n",
        "        self.generator_out_scale = tf.Variable(mag_scale, trainable=False)\n",
        "\n",
        "        # train for a number of epochs on the first run, by default 16, to avoid\n",
        "        # bad results in the beginning of training.\n",
        "        for i in range(self.initial_epochs):\n",
        "            self.train_generator(xy, dy)\n",
        "\n",
        "        self.is_built = True\n",
        "\n",
        "    def generate_gradient(self, x):\n",
        "        '''\n",
        "        Just an MLP, or an affine transformation if the hidden shape in the \n",
        "        constructor is set to be empty.\n",
        "        '''\n",
        "        x = self.generator_hidden(x)\n",
        "        out = self.generator_out(x)\n",
        "        out = out * self.generator_out_scale\n",
        "        return out * self.sg_output_scale\n",
        "\n",
        "    def train_generator(self, x, target):\n",
        "        '''\n",
        "        Gradient descend for the gradient generator. This is called every time a\n",
        "        gradient comes in, although in theory (especially with deeper gradient\n",
        "        generators) once the gradients are modeled sufficiently, it could be OK\n",
        "        to stop training on incoming gradients, thus fully decoupling the lower\n",
        "        parts of the network from the upper parts relative to this SG module.\n",
        "        '''\n",
        "        with tf.GradientTape() as tape:\n",
        "            l2_loss = target - self.generate_gradient(x)\n",
        "            l2_loss = tf.math.reduce_sum(tf.math.square(l2_loss), axis=-1)\n",
        "            # l2_loss = tf.math.sqrt(l2_dist)\n",
        "            grads = tape.gradient(l2_loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "\n",
        "    @tf.custom_gradient\n",
        "    def sg(self, x, y):\n",
        "        '''\n",
        "        In the forward pass it is essentially a no-op (identity). In the\n",
        "        backwards pass it replaces the incoming gradient by a synthetic one.\n",
        "        '''\n",
        "        x = tf.identity(x)\n",
        "\n",
        "        def grad(dy):\n",
        "            # concat x and the label to be inputs for the generator:\n",
        "            xy = self.concat_x_and_y(x, y)\n",
        "\n",
        "            if not self.is_built:\n",
        "                self.build(xy, dy)\n",
        "\n",
        "            # train the generator on the incoming gradient:\n",
        "            for i in range(self.epochs):\n",
        "                self.train_generator(xy, dy)\n",
        "\n",
        "            # return the gradient. The second return value is the gradient for y\n",
        "            # which should be zero since we only need y (labels) to generate the\n",
        "            # synthetic gradients\n",
        "            dy = self.generate_gradient(xy)\n",
        "            return dy, tf.zeros(tf.shape(y))\n",
        "\n",
        "        return x, grad\n",
        "\n",
        "    def __call__(self, x, y):\n",
        "        return self.sg(x, y)\n",
        "\n",
        "    def concat_x_and_y(self, x, y):\n",
        "        '''\n",
        "        Probably an overly complex yet incomplete solution to a rather small\n",
        "        inconvenience.\n",
        "        Inconvenience: The gradient generators take the output of the last \n",
        "        module AND the target/labels of the network as inputs. But those two \n",
        "        tensors can be of different shapes. The obvious solution would be to \n",
        "        manually reshape the targets so they can be concatenated with the \n",
        "        outputs of the past state. But because i wanted this SG module to be as \n",
        "        \"plug-and-play\" as possible, i tried to attempt automatic reshaping.\n",
        "\n",
        "        Should work for 1d->1d, and 1d-sequence -> 1d, possibly 1d seq->seq,\n",
        "        unsure about the rest.\n",
        "        '''\n",
        "        # insert as many dims before the last dim of y to give it the same rank\n",
        "        # as x\n",
        "        amount = tf.math.maximum(tf.rank(x) - tf.rank(y), 0)\n",
        "        new_shape = tf.concat((tf.shape(y)[:-1],\n",
        "                               tf.tile([1], [amount]),\n",
        "                               [tf.shape(y)[-1]]), axis=-1)\n",
        "        y = tf.reshape(y, new_shape)\n",
        "\n",
        "        # tile the added dims such that x and y can be concatenated\n",
        "        # In order to tile only the added dims, i need to set the dimensions \n",
        "        # with a length of 1 (except the last) to the length of the \n",
        "        # corresponding dimensions in x, while setting the rest to 1.\n",
        "        # This is waiting to break.\n",
        "        mask = tf.cast(tf.math.less_equal(tf.shape(y),\n",
        "                                          tf.constant([1])), dtype=tf.int32)\n",
        "        # ignore the last dim\n",
        "        mask = tf.concat([mask[:-1], tf.constant([0])], axis=-1)\n",
        "\n",
        "        zeros_to_ones = tf.math.subtract(\n",
        "            tf.ones(tf.shape(mask), dtype=tf.int32),\n",
        "            mask)\n",
        "        # has ones where there is a one in the shape, now the 1s are set to the\n",
        "        # length in x\n",
        "        mask = tf.math.multiply(mask, tf.shape(x))\n",
        "        # add ones to all other dimensions to preserve their shape\n",
        "        mask = tf.math.add(zeros_to_ones, mask)\n",
        "        # tile\n",
        "        y = tf.tile(y, mask)\n",
        "        return tf.concat((x, y), axis=-1)\n",
        "\n",
        "\n",
        "class FlattenL2D(tf.Module):\n",
        "    \"Flattens the last two dimensions only\"\n",
        "\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        new_shape = tf.concat(\n",
        "            (tf.shape(x)[:-2], [(tf.shape(x)[-1]) * (tf.shape(x)[-2])]),\n",
        "            axis=-1)\n",
        "        return tf.reshape(x, new_shape)\n",
        "\n",
        "\n",
        "initializer = tf.keras.initializers.HeNormal\n",
        "\n",
        "\n",
        "class SentimentAnalysisWithAttention(tf.Module):\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        # Structure and the idea behind it:\n",
        "        # 1: The input sequence is embedded and is positionally encoded.\n",
        "        # 2.1: An MLP block ('query') computes scores for the following\n",
        "        #      attention layer for each entry in the sequence. Ie, it decides\n",
        "        #      which words are worth a closer look.\n",
        "        # 2.2: An attention layer selects n positionally encoded word\n",
        "        #      embeddings from the input sequence based on the learned queries.\n",
        "        # 3: The result is flattened into a tensor of known shape and a number\n",
        "        #    of dense layers compute the final classification.\n",
        "\n",
        "        self.embedding = WordEmbedding(embedding.wv.vectors)\n",
        "        self.batch_norm = tf.keras.layers.BatchNormalization(trainable=True)\n",
        "        self.pos_enc = PositionalEncoding1D()\n",
        "        self.query = MLP_Block([256, 128], initializer=initializer)\n",
        "        self.attention = SequenceCollapseAttention(num_out_entries=9,\n",
        "                                                   initializer=initializer)\n",
        "        self.flatten = FlattenL2D()\n",
        "        self.dense = MLP_Block([512, 256, 128, 64],\n",
        "                               initializer=initializer,\n",
        "                               trainable_batch_norms=True)\n",
        "        self.denseout = MLP_Block([1],\n",
        "                                  initializer=initializer,\n",
        "                                  activation=tf.nn.sigmoid,\n",
        "                                  trainable_batch_norms=True)\n",
        "\n",
        "        # Synthetic gradient modules for the various layers.\n",
        "        self.sg_query = SyntheticGradient(relative_generator_hidden_shapes=[9])\n",
        "        self.sg_attention = SyntheticGradient()\n",
        "        self.sg_dense = SyntheticGradient()\n",
        "\n",
        "    def __call__(self, x, y=tf.constant([]), training=False):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.batch_norm(x, training=training)\n",
        "        q = self.query(x, training=training)\n",
        "        # q = self.sg_query(q, y)           # SG\n",
        "        x = self.attention(x, q)\n",
        "        x = self.flatten(x)\n",
        "        x = self.sg_attention(x, y)         # SG\n",
        "        x = self.dense(x, training=training)\n",
        "        x = self.sg_dense(x, y)             # SG\n",
        "        output = self.denseout(x, training=training)\n",
        "        return output\n",
        "\n",
        "\n",
        "model = SentimentAnalysisWithAttention()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhyMycaj03Tj"
      },
      "source": [
        "class BatchGenerator(tf.keras.utils.Sequence):\n",
        "    '''\n",
        "    Creates batches from the given data, specifically it pads the sequences\n",
        "    per batch only as much as necessary to make every sequence within a batch \n",
        "    be of the same length.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, inputs, labels, padding, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.inputs = inputs\n",
        "        self.padding = padding\n",
        "        # self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.inputs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        max_length = 0\n",
        "        start_index = index * self.batch_size\n",
        "        end_index = start_index + self.batch_size\n",
        "        for i in range(start_index, end_index):\n",
        "            l = len(self.inputs[i])\n",
        "            if l > max_length:\n",
        "                max_length = l\n",
        "\n",
        "        out_x = np.empty([self.batch_size, max_length], dtype='int32')\n",
        "        out_y = np.empty([self.batch_size, 1], dtype='float32')\n",
        "        for i in range(self.batch_size):\n",
        "            out_y[i] = self.labels[start_index + i]\n",
        "            tweet = self.inputs[start_index + i]\n",
        "            l = len(tweet)\n",
        "            l = min(l, max_length)\n",
        "            for j in range(0, l):\n",
        "                out_x[i][j] = tweet[j]\n",
        "            for j in range(l, max_length):\n",
        "                out_x[i][j] = self.padding\n",
        "        return out_x, out_y"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlL1dFpqrZPc"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNJne1GoxT50"
      },
      "source": [
        "def train_validation_loop(model_caller, data_generator, epochs, metrics=[]):\n",
        "    batch_time = -1\n",
        "    for epoch in range(epochs):\n",
        "        start_e = time.time()\n",
        "        start_p = time.time()\n",
        "        num_batches = len(data_generator)\n",
        "        predictions = [None] * num_batches\n",
        "        for b in range(num_batches):\n",
        "            start_b = time.time()\n",
        "\n",
        "            x_batch, y_batch = data_generator[b]\n",
        "            predictions[b] = model_caller(x_batch, y_batch, metrics=metrics)\n",
        "\n",
        "            # progress output\n",
        "            elapsed_t = time.time() - start_b\n",
        "            if batch_time != -1:\n",
        "                batch_time = 0.05 * elapsed_t + 0.95 * batch_time\n",
        "            else:\n",
        "                batch_time = elapsed_t\n",
        "            if int(time.time() - start_p) >= 1 or b == (num_batches - 1):\n",
        "                start_p = time.time()\n",
        "                eta = int((num_batches - b) * batch_time)\n",
        "                ela = int(time.time() - start_e)\n",
        "                out_string = \"\\rEpoch %d/%d,\\tbatch %d/%d,\\telapsed: %d/%ds\" % (\n",
        "                    (epoch + 1), epochs, b + 1, num_batches, ela, ela + eta)\n",
        "                for metric in metrics:\n",
        "                    out_string += \"\\t %s: %f\" % (metric.name,\n",
        "                                                 float(metric.result()))\n",
        "                out_length = len(out_string)\n",
        "                sys.stdout.write(out_string)\n",
        "                sys.stdout.flush()\n",
        "        for metric in metrics:\n",
        "            metric.reset_states()\n",
        "        sys.stdout.write(\"\\n\")\n",
        "    return np.concatenate(predictions)\n",
        "\n",
        "\n",
        "def trainer(model, loss, optimizer):\n",
        "    @tf.function(experimental_relax_shapes=True)\n",
        "    def training_step(x_batch,\n",
        "                      y_batch,\n",
        "                      model=model,\n",
        "                      loss=loss,\n",
        "                      optimizer=optimizer,\n",
        "                      metrics=[]):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(x_batch, y_batch, training=True)\n",
        "            losses = loss(y_batch, predictions)\n",
        "            grads = tape.gradient(losses, model.trainable_variables)\n",
        "\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        for metric in metrics:\n",
        "            metric.update_state(y_batch, predictions)\n",
        "        return predictions\n",
        "\n",
        "    return training_step\n",
        "\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "metrics = (tf.keras.metrics.BinaryCrossentropy(from_logits=True),\n",
        "           tf.keras.metrics.BinaryAccuracy())\n",
        "batch_size = 512\n",
        "epochs = 4\n",
        "\n",
        "padding = embedding.wv.vocab[\"[&END&]\"].index\n",
        "training_generator = BatchGenerator(tokens_train,\n",
        "                                    y_train,\n",
        "                                    padding,\n",
        "                                    batch_size=batch_size)\n",
        "\n",
        "train_validation_loop(trainer(model, loss, optimizer),\n",
        "                      training_generator,\n",
        "                      epochs,\n",
        "                      metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFZA-n8Qrdo1"
      },
      "source": [
        "Testing it on validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yeCFIz6FYx_"
      },
      "source": [
        "def validator(model):\n",
        "    @tf.function(experimental_relax_shapes=True)\n",
        "    def validation_step(x_batch, y_batch, model=model, metrics=[]):\n",
        "        predictions = model(x_batch, y_batch, training=False)\n",
        "        for metric in metrics:\n",
        "            metric.update_state(y_batch, predictions)\n",
        "        return predictions\n",
        "\n",
        "    return validation_step\n",
        "\n",
        "\n",
        "testing_generator = BatchGenerator(tokens_test,\n",
        "                                   y_test,\n",
        "                                   padding,\n",
        "                                   batch_size=batch_size)\n",
        "\n",
        "predictions = train_validation_loop(validator(model),\n",
        "                                    testing_generator,\n",
        "                                    1,\n",
        "                                    metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMqtgxDNgcOK"
      },
      "source": [
        "Get some example results from the the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKR5G4_JVuaR"
      },
      "source": [
        "most_evil_tweet=None\n",
        "most_evil_evilness=1\n",
        "most_cool_tweet=None\n",
        "most_cool_coolness=1\n",
        "most_angelic_tweet=None\n",
        "most_angelic_angelicness=0\n",
        "y_pred = np.concatenate(predictions)\n",
        "for i in range(0,len(y_pred)):\n",
        "    judgement = y_pred[i]\n",
        "    polarity = abs(judgement-0.5)*2\n",
        "\n",
        "    if judgement>=most_angelic_angelicness:\n",
        "        most_angelic_angelicness = judgement\n",
        "        most_angelic_tweet = x_test[i]\n",
        "    if judgement<=most_evil_evilness:\n",
        "        most_evil_evilness = judgement\n",
        "        most_evil_tweet = x_test[i]\n",
        "    if polarity<=most_cool_coolness:\n",
        "        most_cool_coolness = polarity\n",
        "        most_cool_tweet = x_test[i]\n",
        "\n",
        "\n",
        "print(\"The evilest tweet known to humankind:\\n\\t\", most_evil_tweet)\n",
        "print(\"Evilness: \", 1.0-most_evil_evilness)\n",
        "print(\"\\n\")\n",
        "print(\"The most angelic tweet any mortal has ever laid eyes upon:\\n\\t\",\n",
        "      most_angelic_tweet)\n",
        "print(\"Angelicness: \", most_angelic_angelicness)\n",
        "print(\"\\n\")\n",
        "print(\"This tweet is too cool for you, don't read:\\n\\t\", most_cool_tweet)\n",
        "print(\"Coolness: \", 1.0-most_cool_coolness)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
